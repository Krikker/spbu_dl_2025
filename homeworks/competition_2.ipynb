{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13610b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dir exists: True, classes: 18\n",
      "test_dir exists: True, sample files: ['002999fb-803d-45b2-b647-6203e446e475.png', '00479183-ca4e-49c1-927d-f7209ff23145.png', '0048719e-24d9-4a5a-af75-ca62c2e90477.png', '00c99fe0-e1c9-48ef-8d16-1ea380f190cd.png', '00efbf59-d4c4-4d7e-8bc6-86af36367515.png']\n",
      "Loaded 2915 train images across 18 classes: ['Ace', 'Akainu', 'Brook', 'Chopper', 'Crocodile', 'Franky', 'Jinbei', 'Kurohige', 'Law', 'Luffy', 'Mihawk', 'Nami', 'Rayleigh', 'Robin', 'Sanji', 'Shanks', 'Usopp', 'Zoro']\n",
      "Class distribution: {0: 168, 1: 167, 2: 178, 3: 170, 4: 167, 5: 170, 6: 166, 7: 170, 8: 175, 9: 97, 10: 167, 11: 181, 12: 167, 13: 167, 14: 135, 15: 168, 16: 170, 17: 132}\n",
      "Class weights: [0.9639550447463989, 0.9697272181510925, 0.9098002314567566, 0.9526143670082092, 0.9697272181510925] ...\n",
      "Loaded 849 test IDs, sample: ['c41628b1-4781-4392-ac8d-6bfe981f73f9', 'f114acb3-fe18-478b-a19a-1f4cbe098851', 'd952ecfe-750c-44b2-96c2-1cac1a4ee146']\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "C:\\Python312\\Lib\\site-packages\\PIL\\Image.py:981: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 2.4268, Val F1: 0.2298, Val Acc: 0.2419, LR: 0.000050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python312\\Lib\\site-packages\\PIL\\Image.py:981: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/50], Loss: 2.1954, Val F1: 0.3785, Val Acc: 0.3859, LR: 0.000050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python312\\Lib\\site-packages\\PIL\\Image.py:981: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/50], Loss: 2.0442, Val F1: 0.4871, Val Acc: 0.5077, LR: 0.000050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python312\\Lib\\site-packages\\PIL\\Image.py:981: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/50], Loss: 1.9047, Val F1: 0.5382, Val Acc: 0.5489, LR: 0.000049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python312\\Lib\\site-packages\\PIL\\Image.py:981: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/50], Loss: 1.7719, Val F1: 0.6286, Val Acc: 0.6346, LR: 0.000049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python312\\Lib\\site-packages\\PIL\\Image.py:981: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/50], Loss: 1.6526, Val F1: 0.6101, Val Acc: 0.6158, LR: 0.000048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python312\\Lib\\site-packages\\PIL\\Image.py:981: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/50], Loss: 1.5695, Val F1: 0.6813, Val Acc: 0.6861, LR: 0.000048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python312\\Lib\\site-packages\\PIL\\Image.py:981: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/50], Loss: 1.4617, Val F1: 0.7088, Val Acc: 0.7101, LR: 0.000047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python312\\Lib\\site-packages\\PIL\\Image.py:981: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/50], Loss: 1.4303, Val F1: 0.6938, Val Acc: 0.6964, LR: 0.000046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python312\\Lib\\site-packages\\PIL\\Image.py:981: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/50], Loss: 1.2982, Val F1: 0.7139, Val Acc: 0.7153, LR: 0.000045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python312\\Lib\\site-packages\\PIL\\Image.py:981: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/50], Loss: 1.2869, Val F1: 0.7277, Val Acc: 0.7290, LR: 0.000044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python312\\Lib\\site-packages\\PIL\\Image.py:981: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/50], Loss: 1.2445, Val F1: 0.7388, Val Acc: 0.7376, LR: 0.000043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python312\\Lib\\site-packages\\PIL\\Image.py:981: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/50], Loss: 1.1918, Val F1: 0.7357, Val Acc: 0.7427, LR: 0.000042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python312\\Lib\\site-packages\\PIL\\Image.py:981: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/50], Loss: 1.0974, Val F1: 0.7500, Val Acc: 0.7530, LR: 0.000041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python312\\Lib\\site-packages\\PIL\\Image.py:981: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/50], Loss: 1.1070, Val F1: 0.7371, Val Acc: 0.7427, LR: 0.000040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python312\\Lib\\site-packages\\PIL\\Image.py:981: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unfroze base layers for full fine-tune.\n",
      "Epoch [16/50], Loss: 1.0690, Val F1: 0.7547, Val Acc: 0.7581, LR: 0.000010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python312\\Lib\\site-packages\\PIL\\Image.py:981: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/50], Loss: 0.8240, Val F1: 0.8730, Val Acc: 0.8748, LR: 0.000010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python312\\Lib\\site-packages\\PIL\\Image.py:981: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/50], Loss: 0.4885, Val F1: 0.9147, Val Acc: 0.9142, LR: 0.000010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python312\\Lib\\site-packages\\PIL\\Image.py:981: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/50], Loss: 0.3499, Val F1: 0.9212, Val Acc: 0.9211, LR: 0.000010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python312\\Lib\\site-packages\\PIL\\Image.py:981: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/50], Loss: 0.3259, Val F1: 0.9331, Val Acc: 0.9331, LR: 0.000010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python312\\Lib\\site-packages\\PIL\\Image.py:981: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/50], Loss: 0.2974, Val F1: 0.9330, Val Acc: 0.9331, LR: 0.000010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python312\\Lib\\site-packages\\PIL\\Image.py:981: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/50], Loss: 0.2734, Val F1: 0.9292, Val Acc: 0.9297, LR: 0.000010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python312\\Lib\\site-packages\\PIL\\Image.py:981: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/50], Loss: 0.2589, Val F1: 0.9378, Val Acc: 0.9383, LR: 0.000010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python312\\Lib\\site-packages\\PIL\\Image.py:981: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/50], Loss: 0.2550, Val F1: 0.9384, Val Acc: 0.9383, LR: 0.000010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python312\\Lib\\site-packages\\PIL\\Image.py:981: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/50], Loss: 0.2295, Val F1: 0.9414, Val Acc: 0.9417, LR: 0.000010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python312\\Lib\\site-packages\\PIL\\Image.py:981: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26/50], Loss: 0.2430, Val F1: 0.9484, Val Acc: 0.9485, LR: 0.000010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python312\\Lib\\site-packages\\PIL\\Image.py:981: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [27/50], Loss: 0.2336, Val F1: 0.9480, Val Acc: 0.9485, LR: 0.000010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python312\\Lib\\site-packages\\PIL\\Image.py:981: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [28/50], Loss: 0.1827, Val F1: 0.9496, Val Acc: 0.9503, LR: 0.000010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python312\\Lib\\site-packages\\PIL\\Image.py:981: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [29/50], Loss: 0.2077, Val F1: 0.9463, Val Acc: 0.9468, LR: 0.000010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python312\\Lib\\site-packages\\PIL\\Image.py:981: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/50], Loss: 0.2005, Val F1: 0.9587, Val Acc: 0.9588, LR: 0.000010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python312\\Lib\\site-packages\\PIL\\Image.py:981: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [31/50], Loss: 0.1598, Val F1: 0.9551, Val Acc: 0.9554, LR: 0.000010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python312\\Lib\\site-packages\\PIL\\Image.py:981: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [32/50], Loss: 0.2445, Val F1: 0.9408, Val Acc: 0.9417, LR: 0.000010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python312\\Lib\\site-packages\\PIL\\Image.py:981: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [33/50], Loss: 0.1973, Val F1: 0.9520, Val Acc: 0.9520, LR: 0.000010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python312\\Lib\\site-packages\\PIL\\Image.py:981: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [34/50], Loss: 0.2143, Val F1: 0.9604, Val Acc: 0.9605, LR: 0.000010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python312\\Lib\\site-packages\\PIL\\Image.py:981: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [35/50], Loss: 0.2020, Val F1: 0.9535, Val Acc: 0.9537, LR: 0.000010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python312\\Lib\\site-packages\\PIL\\Image.py:981: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [36/50], Loss: 0.2440, Val F1: 0.9605, Val Acc: 0.9605, LR: 0.000010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python312\\Lib\\site-packages\\PIL\\Image.py:981: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [37/50], Loss: 0.2011, Val F1: 0.9480, Val Acc: 0.9485, LR: 0.000010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python312\\Lib\\site-packages\\PIL\\Image.py:981: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [38/50], Loss: 0.1802, Val F1: 0.9518, Val Acc: 0.9520, LR: 0.000010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python312\\Lib\\site-packages\\PIL\\Image.py:981: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [39/50], Loss: 0.1928, Val F1: 0.9639, Val Acc: 0.9640, LR: 0.000010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python312\\Lib\\site-packages\\PIL\\Image.py:981: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [40/50], Loss: 0.2216, Val F1: 0.9566, Val Acc: 0.9571, LR: 0.000010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python312\\Lib\\site-packages\\PIL\\Image.py:981: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [41/50], Loss: 0.1561, Val F1: 0.9552, Val Acc: 0.9554, LR: 0.000010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python312\\Lib\\site-packages\\PIL\\Image.py:981: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [42/50], Loss: 0.1766, Val F1: 0.9553, Val Acc: 0.9554, LR: 0.000010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python312\\Lib\\site-packages\\PIL\\Image.py:981: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [43/50], Loss: 0.1949, Val F1: 0.9600, Val Acc: 0.9605, LR: 0.000010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python312\\Lib\\site-packages\\PIL\\Image.py:981: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [44/50], Loss: 0.1917, Val F1: 0.9621, Val Acc: 0.9623, LR: 0.000010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python312\\Lib\\site-packages\\PIL\\Image.py:981: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [45/50], Loss: 0.1985, Val F1: 0.9607, Val Acc: 0.9605, LR: 0.000010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python312\\Lib\\site-packages\\PIL\\Image.py:981: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [46/50], Loss: 0.1811, Val F1: 0.9590, Val Acc: 0.9588, LR: 0.000010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python312\\Lib\\site-packages\\PIL\\Image.py:981: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [47/50], Loss: 0.1334, Val F1: 0.9692, Val Acc: 0.9691, LR: 0.000010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python312\\Lib\\site-packages\\PIL\\Image.py:981: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [48/50], Loss: 0.1630, Val F1: 0.9605, Val Acc: 0.9605, LR: 0.000010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python312\\Lib\\site-packages\\PIL\\Image.py:981: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [49/50], Loss: 0.1803, Val F1: 0.9602, Val Acc: 0.9605, LR: 0.000010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python312\\Lib\\site-packages\\PIL\\Image.py:981: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/50], Loss: 0.1963, Val F1: 0.9657, Val Acc: 0.9657, LR: 0.000010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python312\\Lib\\site-packages\\PIL\\Image.py:981: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated submission.csv saved!\n",
      "Sample:                                      id  label\n",
      "0  c41628b1-4781-4392-ac8d-6bfe981f73f9     10\n",
      "1  f114acb3-fe18-478b-a19a-1f4cbe098851      7\n",
      "2  d952ecfe-750c-44b2-96c2-1cac1a4ee146      2\n",
      "3  2c14ec77-44ca-4b3c-b470-96286411c617     14\n",
      "4  712c3ce9-750a-4cc4-8f94-f8033c31cb2c      0\n",
      "Pred class dist: [53 50 45 48 50 51 50 46 41 32 55 50 53 50 43 48 42 42]\n",
      "Final best Val F1: 0.9692, Acc: 0.9657\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.models import resnet50\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from collections import Counter\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "\n",
    "root_dir = r'C:\\Users\\KRIKKER\\Course_py\\spbu_dl\\one-piece\\splitted'\n",
    "train_dir = os.path.join(root_dir, 'train')\n",
    "test_dir = os.path.join(root_dir, 'test')\n",
    "print(f\"train_dir exists: {os.path.exists(train_dir)}, classes: {len(os.listdir(train_dir)) if os.path.exists(train_dir) else 'N/A'}\")\n",
    "print(f\"test_dir exists: {os.path.exists(test_dir)}, sample files: {os.listdir(test_dir)[:5] if os.path.exists(test_dir) else 'N/A'}\")\n",
    "\n",
    "class_to_idx = {\n",
    "    'Ace': 0, 'Akainu': 1, 'Brook': 2, 'Chopper': 3, 'Crocodile': 4, 'Franky': 5, 'Jinbei': 6, 'Kurohige': 7,\n",
    "    'Law': 8, 'Luffy': 9, 'Mihawk': 10, 'Nami': 11, 'Rayleigh': 12, 'Robin': 13, 'Sanji': 14, 'Shanks': 15, 'Usopp': 16, 'Zoro': 17\n",
    "}\n",
    "\n",
    "class TestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, submission_csv, test_dir, transform=None):\n",
    "        self.sub_df = pd.read_csv(submission_csv, dtype={'id': str})\n",
    "        self.test_dir = test_dir\n",
    "        self.transform = transform\n",
    "        self.possible_exts = ['.jpg', '.jpeg', '.png']\n",
    "        print(f\"Loaded {len(self.sub_df)} test IDs, sample: {self.sub_df['id'].head(3).tolist()}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sub_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.sub_df.iloc[idx]\n",
    "        image_id = str(row['id'])\n",
    "        \n",
    "        img_path = None\n",
    "        for ext in self.possible_exts:\n",
    "            candidate = os.path.join(self.test_dir, image_id + ext)\n",
    "            if os.path.exists(candidate):\n",
    "                img_path = candidate\n",
    "                break\n",
    "        if img_path is None:\n",
    "            print(f\"Warning: No image for {image_id}. Sample test files: {os.listdir(self.test_dir)[:5]}\")\n",
    "            raise FileNotFoundError(f\"Image not found for {image_id}\")\n",
    "        \n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, image_id\n",
    "\n",
    "class ResNet50(nn.Module):\n",
    "    def __init__(self, num_classes=18):\n",
    "        super(ResNet50, self).__init__()\n",
    "        self.model = resnet50(pretrained=True)\n",
    "        self.model.fc = nn.Linear(self.model.fc.in_features, num_classes)\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.model.fc.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.75, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.15),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "    transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.5)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_dataset_full = ImageFolder(train_dir, transform=train_transform)\n",
    "print(f\"Loaded {len(train_dataset_full)} train images across {len(train_dataset_full.classes)} classes: {train_dataset_full.classes}\")\n",
    "\n",
    "expected_classes = sorted(class_to_idx.keys())\n",
    "if train_dataset_full.classes != expected_classes:\n",
    "    print(\"Warning: Class order mismatch! Remapping targets...\")\n",
    "    new_targets = [class_to_idx[train_dataset_full.classes[label]] for label in train_dataset_full.targets]\n",
    "    train_dataset_full.targets = new_targets\n",
    "    print(\"Remapped OK.\")\n",
    "\n",
    "train_labels = train_dataset_full.targets\n",
    "class_counts = Counter(train_labels)\n",
    "print(\"Class distribution:\", dict(sorted(class_counts.items())))\n",
    "total_samples = len(train_labels)\n",
    "class_weights = {label: total_samples / (len(class_counts) * count) for label, count in class_counts.items()}\n",
    "weights_tensor = torch.tensor([class_weights.get(i, 1.0) for i in range(18)]).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Class weights:\", weights_tensor.tolist()[:5], \"...\")\n",
    "\n",
    "train_size = int(0.8 * len(train_dataset_full))\n",
    "val_size = len(train_dataset_full) - train_size\n",
    "train_dataset, val_dataset = random_split(train_dataset_full, [train_size, val_size])\n",
    "val_dataset.dataset.transform = test_transform\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=12, shuffle=True, num_workers=0)  # Smaller batch for ResNet50\n",
    "val_loader = DataLoader(val_dataset, batch_size=12, shuffle=False, num_workers=0)\n",
    "\n",
    "submission_csv = r'C:\\Users\\KRIKKER\\Course_py\\spbu_dl\\one-piece\\submission.csv'\n",
    "test_dataset = TestDataset(submission_csv, test_dir, test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=12, shuffle=False, num_workers=0)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "model = ResNet50(num_classes=18).to(device)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.00005, weight_decay=0.01)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-6)\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none', weight=weights_tensor)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = (1 - pt) ** self.gamma * ce_loss\n",
    "        if self.alpha is not None:\n",
    "            focal_loss = self.alpha[targets] * focal_loss\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        return focal_loss.sum()\n",
    "\n",
    "criterion = FocalLoss(alpha=weights_tensor, gamma=2.0)\n",
    "\n",
    "def mixup_data(x, y, alpha=0.2):\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size).to(device)\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "num_epochs = 50\n",
    "best_f1 = 0.0\n",
    "patience = 12\n",
    "counter = 0\n",
    "label_smoothing = 0.1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        if np.random.rand() < 0.5:\n",
    "            images, targets_a, targets_b, lam = mixup_data(images, labels, alpha=0.2)\n",
    "            outputs = model(images)\n",
    "            loss = lam * criterion(outputs, targets_a) + (1 - lam) * criterion(outputs, targets_b)\n",
    "        else:\n",
    "            outputs = model(images)\n",
    "            targets = F.one_hot(labels, num_classes=18).float().to(device)\n",
    "            targets = targets * (1 - label_smoothing) + (label_smoothing / 18.0)\n",
    "            loss = F.kl_div(F.log_softmax(outputs, dim=1), targets, reduction='batchmean')\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Grad clip\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    if epoch == 15:\n",
    "        for param in model.model.parameters():\n",
    "            param.requires_grad = True\n",
    "        print(\"Unfroze base layers for full fine-tune.\")\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=0.00001)\n",
    "\n",
    "    model.eval()\n",
    "    val_preds, val_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            val_preds.extend(preds.cpu().numpy())\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    val_f1 = f1_score(val_labels, val_preds, average='weighted')\n",
    "    val_acc = accuracy_score(val_labels, val_preds)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, Val F1: {val_f1:.4f}, Val Acc: {val_acc:.4f}, LR: {optimizer.param_groups[0]['lr']:.6f}')\n",
    "    \n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1\n",
    "        counter = 0\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(f'Early stopping at epoch {epoch+1}')\n",
    "            break\n",
    "\n",
    "model.load_state_dict(torch.load('best_model.pth', weights_only=True))\n",
    "model.eval()\n",
    "predictions = []\n",
    "ids = []\n",
    "with torch.no_grad():\n",
    "    for images, image_ids in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        _, pred_labels = torch.max(outputs, 1)\n",
    "        predictions.extend(pred_labels.cpu().numpy())\n",
    "        ids.extend(image_ids)\n",
    "\n",
    "sub_df = pd.read_csv(submission_csv, dtype={'id': str})\n",
    "sub_df['label'] = predictions\n",
    "sub_df.to_csv('submission.csv', index=False)\n",
    "print('Updated submission.csv saved!')\n",
    "print(f\"Sample: {sub_df.head()}\")\n",
    "print(f\"Pred class dist: {np.bincount(predictions)}\")\n",
    "print(f\"Final best Val F1: {best_f1:.4f}, Acc: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d27f3bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
