{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b69b9e",
   "metadata": {},
   "source": [
    "# Задание 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d956137f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Запуск BPE\n",
      "Загрузка новостного корпуса из news.csv\n",
      "Загружено 21673 новостных текстов\n",
      "Загрузка корпуса Пушкина из pushkin/texts.zip\n",
      "Целевой размер словаря=5000, минимальная частота=3\n",
      "Типов слов после отсечения: 107582\n",
      "Начальный размер словаря: 236\n",
      "Шаг 100: объединена пара ('ст', 'о') → сто | словарь=336 | 6.72% | осталось=4664\n",
      "Шаг 200: объединена пара (' ', 'Б') →  Б | словарь=436 | 8.72% | осталось=4564\n",
      "Шаг 300: объединена пара (' ра', 'бо') →  рабо | словарь=536 | 10.72% | осталось=4464\n",
      "Шаг 400: объединена пара (' ', 'З') →  З | словарь=636 | 12.72% | осталось=4364\n",
      "Шаг 500: объединена пара ('ж', 'ду') → жду | словарь=736 | 14.72% | осталось=4264\n",
      "Шаг 600: объединена пара ('ме', 'ни') → мени | словарь=836 | 16.72% | осталось=4164\n",
      "Шаг 700: объединена пара ('сп', 'е') → спе | словарь=936 | 18.72% | осталось=4064\n",
      "Шаг 800: объединена пара (' г', 'руп') →  груп | словарь=1036 | 20.72% | осталось=3964\n",
      "Шаг 900: объединена пара (' тыся', 'чи') →  тысячи | словарь=1136 | 22.72% | осталось=3864\n",
      "Шаг 1000: объединена пара ('ро', 'н') → рон | словарь=1236 | 24.72% | осталось=3764\n",
      "Шаг 1100: объединена пара (' ж', 'ур') →  жур | словарь=1336 | 26.72% | осталось=3664\n",
      "Шаг 1200: объединена пара (' в', 'ла') →  вла | словарь=1436 | 28.72% | осталось=3564\n",
      "Шаг 1300: объединена пара (' под', 'держ') →  поддерж | словарь=1536 | 30.72% | осталось=3464\n",
      "Шаг 1400: объединена пара (' чет', 'ыре') →  четыре | словарь=1636 | 32.72% | осталось=3364\n",
      "Шаг 1500: объединена пара ('ру', 'ет') → рует | словарь=1736 | 34.72% | осталось=3264\n",
      "Шаг 1600: объединена пара (' оче', 'ред') →  очеред | словарь=1836 | 36.72% | осталось=3164\n",
      "Шаг 1700: объединена пара (' об', 'на') →  обна | словарь=1936 | 38.72% | осталось=3064\n",
      "Шаг 1800: объединена пара ('но', 'логи') → нологи | словарь=2036 | 40.72% | осталось=2964\n",
      "Шаг 1900: объединена пара ('ме', 'ст') → мест | словарь=2136 | 42.72% | осталось=2864\n",
      "Шаг 2000: объединена пара ('сколь', 'ку') → скольку | словарь=2236 | 44.72% | осталось=2764\n",
      "Шаг 2100: объединена пара (' эти', 'х') →  этих | словарь=2336 | 46.72% | осталось=2664\n",
      "Шаг 2200: объединена пара (' боль', 'шин') →  большин | словарь=2436 | 48.72% | осталось=2564\n",
      "Шаг 2300: объединена пара (' т', 'ова') →  това | словарь=2536 | 50.72% | осталось=2464\n",
      "Шаг 2400: объединена пара (' с', 'лож') →  слож | словарь=2636 | 52.72% | осталось=2364\n",
      "Шаг 2500: объединена пара (' муж', 'чи') →  мужчи | словарь=2736 | 54.72% | осталось=2264\n",
      "Шаг 2600: объединена пара (' ', 'жа') →  жа | словарь=2836 | 56.72% | осталось=2164\n",
      "Шаг 2700: объединена пара (' ха', 'рак') →  харак | словарь=2936 | 58.72% | осталось=2064\n",
      "Шаг 2800: объединена пара (' в', 'лия') →  влия | словарь=3036 | 60.72% | осталось=1964\n",
      "Шаг 2900: объединена пара ('ти', 'ли') → тили | словарь=3136 | 62.72% | осталось=1864\n",
      "Шаг 3000: объединена пара (' пра', 'воохра') →  правоохра | словарь=3236 | 64.72% | осталось=1764\n",
      "Шаг 3100: объединена пара ('З', 'а') → За | словарь=3336 | 66.72% | осталось=1664\n",
      "Шаг 3200: объединена пара (' чу', 'в') →  чув | словарь=3436 | 68.72% | осталось=1564\n",
      "Шаг 3300: объединена пара ('К', 'С') → КС | словарь=3536 | 70.72% | осталось=1464\n",
      "Шаг 3400: объединена пара (' при', 'водит') →  приводит | словарь=3636 | 72.72% | осталось=1364\n",
      "Шаг 3500: объединена пара ('Н', 'е') → Не | словарь=3736 | 74.72% | осталось=1264\n",
      "Шаг 3600: объединена пара (' пери', 'о') →  перио | словарь=3836 | 76.72% | осталось=1164\n",
      "Шаг 3700: объединена пара ('г', 'ер') → гер | словарь=3936 | 78.72% | осталось=1064\n",
      "Шаг 3800: объединена пара (' газе', 'та') →  газета | словарь=4036 | 80.72% | осталось=964\n",
      "Шаг 3900: объединена пара (' эк', 'с') →  экс | словарь=4136 | 82.72% | осталось=864\n",
      "Шаг 4000: объединена пара (' На', 'пример') →  Например | словарь=4236 | 84.72% | осталось=764\n",
      "Шаг 4100: объединена пара (' Дон', 'ба') →  Донба | словарь=4336 | 86.72% | осталось=664\n",
      "Шаг 4200: объединена пара ('сте', 'п') → степ | словарь=4436 | 88.72% | осталось=564\n",
      "Шаг 4300: объединена пара ('ко', 'му') → кому | словарь=4536 | 90.72% | осталось=464\n",
      "Шаг 4400: объединена пара (' страна', 'ми') →  странами | словарь=4636 | 92.72% | осталось=364\n",
      "Шаг 4500: объединена пара (' ', '№') →  № | словарь=4736 | 94.72% | осталось=264\n",
      "Шаг 4600: объединена пара ('са', 'ми') → сами | словарь=4836 | 96.72% | осталось=164\n",
      "Шаг 4700: объединена пара (' Влади', 'ми') →  Владими | словарь=4936 | 98.72% | осталось=64\n",
      "Шаг 4764: объединена пара (' социаль', 'ных') →  социальных | словарь=5000 | 100.00% | осталось=0\n",
      "Обучение завершено | объединений=4764 | словарь=5000\n",
      "Степень сжатия: 0.3242320819112628\n",
      "Токенов на слово: 2.419524378520668\n",
      "Токенов на частое слово: 1.9205465803145683\n",
      "Доля неиспользуемых токенов: 0.3546\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# Предтокенизация\n",
    "PRETOKEN_PATTERN = re.compile(\n",
    "    r\"'s|'t|'re|'ve|'m|'ll|'d| ?[A-Za-zА-Яа-яЁё]+| ?\\d+| ?[^\\sA-Za-zА-Яа-яЁё\\d]+|\\s+(?!\\S)|\\s+\"\n",
    ")\n",
    "\n",
    "SPECIAL_TOKENS = [\"<pad>\", \"<unk>\", \"<bos>\", \"<eos>\"]\n",
    "\n",
    "def pretokenize(text: str):\n",
    "    return PRETOKEN_PATTERN.findall(text)\n",
    "\n",
    "# Реализация BPE\n",
    "class BPETokenizer:\n",
    "    def __init__(self, vocab_size=3000, min_freq=2):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.min_freq = min_freq\n",
    "        self.bpe_merges = []\n",
    "        self.token2id = {}\n",
    "        self.id2token = {}\n",
    "\n",
    "    def train(self, corpus):\n",
    "        print(f\"Целевой размер словаря={self.vocab_size}, минимальная частота={self.min_freq}\")\n",
    "\n",
    "        word_freq = Counter()\n",
    "        for i, text in enumerate(corpus):\n",
    "            for tok in pretokenize(text):\n",
    "                word_freq[tuple(tok)] += 1\n",
    "\n",
    "        word_freq = Counter({w: c for w, c in word_freq.items() if c >= self.min_freq})\n",
    "        print(f\"Типов слов после отсечения: {len(word_freq)}\")\n",
    "\n",
    "        # Инициализация словаря\n",
    "        vocab = set()\n",
    "        for w in word_freq:\n",
    "            vocab.update(w)\n",
    "        vocab |= set(SPECIAL_TOKENS)\n",
    "\n",
    "        print(f\"Начальный размер словаря: {len(vocab)}\")\n",
    "\n",
    "        # Построение статистики пар\n",
    "        pair_freq = defaultdict(int)\n",
    "        for word, freq in word_freq.items():\n",
    "            for i in range(len(word) - 1):\n",
    "                pair_freq[(word[i], word[i + 1])] += freq\n",
    "\n",
    "        step = 0\n",
    "        while len(vocab) < self.vocab_size and pair_freq:\n",
    "            # Лучшая пара без полной сортировки\n",
    "            best_pair = max(pair_freq.items(), key=lambda x: x[1])[0]\n",
    "            step += 1\n",
    "\n",
    "            new_word_freq = Counter()\n",
    "            pair_freq = defaultdict(int)\n",
    "\n",
    "            for word, freq in word_freq.items():\n",
    "                new_word = []\n",
    "                i = 0\n",
    "                while i < len(word):\n",
    "                    if i < len(word) - 1 and (word[i], word[i + 1]) == best_pair:\n",
    "                        new_word.append(word[i] + word[i + 1])\n",
    "                        i += 2\n",
    "                    else:\n",
    "                        new_word.append(word[i])\n",
    "                        i += 1\n",
    "                new_word = tuple(new_word)\n",
    "                new_word_freq[new_word] += freq\n",
    "\n",
    "                for j in range(len(new_word) - 1):\n",
    "                    pair_freq[(new_word[j], new_word[j + 1])] += freq\n",
    "\n",
    "            word_freq = new_word_freq\n",
    "            merged = best_pair[0] + best_pair[1]\n",
    "            vocab.add(merged)\n",
    "            self.bpe_merges.append(best_pair)\n",
    "\n",
    "            if step % 100 == 0 or len(vocab) >= self.vocab_size:\n",
    "                progress = 100 * len(vocab) / self.vocab_size\n",
    "                remaining = self.vocab_size - len(vocab)\n",
    "                print(f\"Шаг {step}: объединена пара {best_pair} → {merged} | словарь={len(vocab)} | {progress:.2f}% | осталось={remaining}\")\n",
    "\n",
    "        print(f\"Обучение завершено | объединений={len(self.bpe_merges)} | словарь={len(vocab)}\")\n",
    "\n",
    "        all_tokens = sorted(vocab)\n",
    "        self.token2id = {t: i for i, t in enumerate(all_tokens)}\n",
    "        self.id2token = {i: t for t, i in self.token2id.items()}\n",
    "\n",
    "    def encode(self, text: str):\n",
    "        output = []\n",
    "        for tok in pretokenize(text):\n",
    "            symbols = list(tok)\n",
    "            for a, b in self.bpe_merges:\n",
    "                i = 0\n",
    "                while i < len(symbols) - 1:\n",
    "                    if symbols[i] == a and symbols[i + 1] == b:\n",
    "                        symbols[i:i + 2] = [a + b]\n",
    "                    else:\n",
    "                        i += 1\n",
    "            for s in symbols:\n",
    "                output.append(self.token2id.get(s, self.token2id[\"<unk>\"]))\n",
    "        return output\n",
    "\n",
    "# Загрузка корпусов\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\n+', '\\n', text)\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def load_pushkin(zip_path):\n",
    "    print(f\"Загрузка корпуса Пушкина из {zip_path}\")\n",
    "    texts = []\n",
    "    with zipfile.ZipFile(zip_path, 'r') as z:\n",
    "        for name in z.namelist():\n",
    "            with z.open(name) as f:\n",
    "                texts.append(f.read().decode(\"utf-8\"))\n",
    "    texts = [clean_text(t) for t in texts]\n",
    "    return texts\n",
    "\n",
    "\n",
    "def load_news(path, max_items=None):\n",
    "    print(f\"Загрузка новостного корпуса из {path}\")\n",
    "    df = pd.read_csv(path)\n",
    "    texts = df['text'].dropna().tolist()\n",
    "    if max_items is not None:\n",
    "        texts = texts[:max_items]\n",
    "    texts = [clean_text(t) for t in texts]\n",
    "    print(f\"Загружено {len(texts)} новостных текстов\")\n",
    "    return texts\n",
    "\n",
    "# Метрики\n",
    "def compression_ratio(tokens, text):\n",
    "    return len(tokens) / len(text.encode(\"utf-8\"))\n",
    "\n",
    "def tokens_per_word(tokenizer, texts):\n",
    "    total_tokens = 0\n",
    "    total_words = 0\n",
    "    for t in texts:\n",
    "        words = re.findall(r\"[А-Яа-яЁё]+\", t)\n",
    "        for w in words:\n",
    "            total_tokens += len(tokenizer.encode(w))\n",
    "            total_words += 1\n",
    "    return total_tokens / total_words\n",
    "\n",
    "def top_freq_tokens_per_word(tokenizer, texts, top=0.1):\n",
    "    freq = Counter()\n",
    "    for t in texts:\n",
    "        for w in re.findall(r\"[А-Яа-яЁё]+\", t):\n",
    "            freq[w] += 1\n",
    "    cutoff = int(len(freq) * top)\n",
    "    common = set(w for w, _ in freq.most_common(cutoff))\n",
    "\n",
    "    tok = 0\n",
    "    cnt = 0\n",
    "    for w in common:\n",
    "        tok += len(tokenizer.encode(w)) * freq[w]\n",
    "        cnt += freq[w]\n",
    "    return tok / cnt\n",
    "\n",
    "def unused_tokens(tokenizer, texts):\n",
    "    used = Counter()\n",
    "    for t in texts:\n",
    "        for i in tokenizer.encode(t):\n",
    "            used[i] += 1\n",
    "    return 1 - len(used) / len(tokenizer.token2id)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Запуск BPE\")\n",
    "\n",
    "    news_texts = load_news(\"news.csv\", max_items=200000)\n",
    "    pushkin_texts = load_pushkin(\"pushkin/texts.zip\")\n",
    "\n",
    "    tokenizer = BPETokenizer(vocab_size=5000, min_freq=3)\n",
    "    tokenizer.train(news_texts)\n",
    "\n",
    "    sample = pushkin_texts[0]\n",
    "    print(\"Степень сжатия:\", compression_ratio(tokenizer.encode(sample), sample))\n",
    "    print(\"Токенов на слово:\", tokens_per_word(tokenizer, pushkin_texts))\n",
    "    print(\"Токенов на частое слово:\", top_freq_tokens_per_word(tokenizer, pushkin_texts))\n",
    "    print(\"Доля неиспользуемых токенов:\", unused_tokens(tokenizer, pushkin_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8859374",
   "metadata": {},
   "source": [
    "# Задание 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03d3e303",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Подготовка датасета и модели на cuda\n",
      "Создание TokenDataset из news_texts\n",
      "Токенизировано 500/10000 текстов\n",
      "Токенизировано 1000/10000 текстов\n",
      "Токенизировано 1500/10000 текстов\n",
      "Токенизировано 2000/10000 текстов\n",
      "Токенизировано 2500/10000 текстов\n",
      "Токенизировано 3000/10000 текстов\n",
      "Токенизировано 3500/10000 текстов\n",
      "Токенизировано 4000/10000 текстов\n",
      "Токенизировано 4500/10000 текстов\n",
      "Токенизировано 5000/10000 текстов\n",
      "Токенизировано 5500/10000 текстов\n",
      "Токенизировано 6000/10000 текстов\n",
      "Токенизировано 6500/10000 текстов\n",
      "Токенизировано 7000/10000 текстов\n",
      "Токенизировано 7500/10000 текстов\n",
      "Токенизировано 8000/10000 текстов\n",
      "Токенизировано 8500/10000 текстов\n",
      "Токенизировано 9000/10000 текстов\n",
      "Токенизировано 9500/10000 текстов\n",
      "Токенизация завершена. Всего токенов: 5193217\n",
      "Подготовлено 5193167 последовательностей длиной 50\n",
      "Инициализация модели BPERNN\n",
      "Начало обучения на cuda, максимум шагов: 100000\n",
      "Эпоха 1, Шаг 1, loss=8.5214\n",
      "Эпоха 1, Шаг 51, loss=7.3146\n",
      "Эпоха 1, Шаг 101, loss=7.2606\n",
      "Эпоха 1, Шаг 151, loss=7.2231\n",
      "Эпоха 1, Шаг 201, loss=7.0931\n",
      "Эпоха 1, Шаг 251, loss=7.0005\n",
      "Эпоха 1, Шаг 301, loss=6.8792\n",
      "Эпоха 1, Шаг 351, loss=6.6448\n",
      "Эпоха 1, Шаг 401, loss=6.5939\n",
      "Эпоха 1, Шаг 451, loss=6.4156\n",
      "Эпоха 1, Шаг 501, loss=6.2202\n",
      "Эпоха 1, Шаг 551, loss=6.2410\n",
      "Эпоха 1, Шаг 601, loss=6.0952\n",
      "Эпоха 1, Шаг 651, loss=6.1042\n",
      "Эпоха 1, Шаг 701, loss=6.0831\n",
      "Эпоха 1, Шаг 751, loss=6.0958\n",
      "Эпоха 1, Шаг 801, loss=5.9737\n",
      "Эпоха 1, Шаг 851, loss=5.9231\n",
      "Эпоха 1, Шаг 901, loss=5.8919\n",
      "Эпоха 1, Шаг 951, loss=5.8560\n",
      "Эпоха 1, Шаг 1001, loss=5.8296\n",
      "Эпоха 1, Шаг 1051, loss=5.7640\n",
      "Эпоха 1, Шаг 1101, loss=5.7098\n",
      "Эпоха 1, Шаг 1151, loss=5.7201\n",
      "Эпоха 1, Шаг 1201, loss=5.6511\n",
      "Эпоха 1, Шаг 1251, loss=5.5655\n",
      "Эпоха 1, Шаг 1301, loss=5.6226\n",
      "Эпоха 1, Шаг 1351, loss=5.6267\n",
      "Эпоха 1, Шаг 1401, loss=5.6497\n",
      "Эпоха 1, Шаг 1451, loss=5.5262\n",
      "Эпоха 1, Шаг 1501, loss=5.5016\n",
      "Эпоха 1, Шаг 1551, loss=5.3929\n",
      "Эпоха 1, Шаг 1601, loss=5.4837\n",
      "Эпоха 1, Шаг 1651, loss=5.4199\n",
      "Эпоха 1, Шаг 1701, loss=5.4062\n",
      "Эпоха 1, Шаг 1751, loss=5.5326\n",
      "Эпоха 1, Шаг 1801, loss=5.3832\n",
      "Эпоха 1, Шаг 1851, loss=5.3550\n",
      "Эпоха 1, Шаг 1901, loss=5.3266\n",
      "Эпоха 1, Шаг 1951, loss=5.2833\n",
      "Эпоха 1, Шаг 2001, loss=5.1659\n",
      "Эпоха 1, Шаг 2051, loss=5.2795\n",
      "Эпоха 1, Шаг 2101, loss=5.2143\n",
      "Эпоха 1, Шаг 2151, loss=5.3223\n",
      "Эпоха 1, Шаг 2201, loss=5.2235\n",
      "Эпоха 1, Шаг 2251, loss=5.1847\n",
      "Эпоха 1, Шаг 2301, loss=5.1376\n",
      "Эпоха 1, Шаг 2351, loss=5.1139\n",
      "Эпоха 1, Шаг 2401, loss=5.1491\n",
      "Эпоха 1, Шаг 2451, loss=5.0741\n",
      "Эпоха 1, Шаг 2501, loss=5.1862\n",
      "Эпоха 1, Шаг 2551, loss=5.1180\n",
      "Эпоха 1, Шаг 2601, loss=5.0713\n",
      "Эпоха 1, Шаг 2651, loss=5.0070\n",
      "Эпоха 1, Шаг 2701, loss=4.9905\n",
      "Эпоха 1, Шаг 2751, loss=5.0139\n",
      "Эпоха 1, Шаг 2801, loss=4.9752\n",
      "Эпоха 1, Шаг 2851, loss=5.0686\n",
      "Эпоха 1, Шаг 2901, loss=4.8766\n",
      "Эпоха 1, Шаг 2951, loss=4.9647\n",
      "Эпоха 1, Шаг 3001, loss=4.9275\n",
      "Эпоха 1, Шаг 3051, loss=4.9307\n",
      "Эпоха 1, Шаг 3101, loss=4.9475\n",
      "Эпоха 1, Шаг 3151, loss=4.9322\n",
      "Эпоха 1, Шаг 3201, loss=4.9081\n",
      "Эпоха 1, Шаг 3251, loss=4.8763\n",
      "Эпоха 1, Шаг 3301, loss=4.8547\n",
      "Эпоха 1, Шаг 3351, loss=4.7503\n",
      "Эпоха 1, Шаг 3401, loss=4.8148\n",
      "Эпоха 1, Шаг 3451, loss=4.8476\n",
      "Эпоха 1, Шаг 3501, loss=4.8410\n",
      "Эпоха 1, Шаг 3551, loss=4.8589\n",
      "Эпоха 1, Шаг 3601, loss=4.8629\n",
      "Эпоха 1, Шаг 3651, loss=4.6679\n",
      "Эпоха 1, Шаг 3701, loss=4.8457\n",
      "Эпоха 1, Шаг 3751, loss=4.7801\n",
      "Эпоха 1, Шаг 3801, loss=4.6273\n",
      "Эпоха 1, Шаг 3851, loss=4.7254\n",
      "Эпоха 1, Шаг 3901, loss=4.6782\n",
      "Эпоха 1, Шаг 3951, loss=4.8239\n",
      "Эпоха 1, Шаг 4001, loss=4.8136\n",
      "Эпоха 1, Шаг 4051, loss=4.7574\n",
      "Эпоха 1, Шаг 4101, loss=4.7010\n",
      "Эпоха 1, Шаг 4151, loss=4.7971\n",
      "Эпоха 1, Шаг 4201, loss=4.6360\n",
      "Эпоха 1, Шаг 4251, loss=4.7078\n",
      "Эпоха 1, Шаг 4301, loss=4.6776\n",
      "Эпоха 1, Шаг 4351, loss=4.5539\n",
      "Эпоха 1, Шаг 4401, loss=4.6015\n",
      "Эпоха 1, Шаг 4451, loss=4.6153\n",
      "Эпоха 1, Шаг 4501, loss=4.6695\n",
      "Эпоха 1, Шаг 4551, loss=4.6171\n",
      "Эпоха 1, Шаг 4601, loss=4.5831\n",
      "Эпоха 1, Шаг 4651, loss=4.5745\n",
      "Эпоха 1, Шаг 4701, loss=4.6017\n",
      "Эпоха 1, Шаг 4751, loss=4.5660\n",
      "Эпоха 1, Шаг 4801, loss=4.5910\n",
      "Эпоха 1, Шаг 4851, loss=4.4867\n",
      "Эпоха 1, Шаг 4901, loss=4.6329\n",
      "Эпоха 1, Шаг 4951, loss=4.6420\n",
      "Эпоха 1, Шаг 5001, loss=4.4002\n",
      "Эпоха 1, Шаг 5051, loss=4.5537\n",
      "Эпоха 1, Шаг 5101, loss=4.5375\n",
      "Эпоха 1, Шаг 5151, loss=4.5141\n",
      "Эпоха 1, Шаг 5201, loss=4.5390\n",
      "Эпоха 1, Шаг 5251, loss=4.5146\n",
      "Эпоха 1, Шаг 5301, loss=4.4837\n",
      "Эпоха 1, Шаг 5351, loss=4.4717\n",
      "Эпоха 1, Шаг 5401, loss=4.5715\n",
      "Эпоха 1, Шаг 5451, loss=4.4274\n",
      "Эпоха 1, Шаг 5501, loss=4.5254\n",
      "Эпоха 1, Шаг 5551, loss=4.4656\n",
      "Эпоха 1, Шаг 5601, loss=4.4433\n",
      "Эпоха 1, Шаг 5651, loss=4.5228\n",
      "Эпоха 1, Шаг 5701, loss=4.3822\n",
      "Эпоха 1, Шаг 5751, loss=4.4840\n",
      "Эпоха 1, Шаг 5801, loss=4.3842\n",
      "Эпоха 1, Шаг 5851, loss=4.5318\n",
      "Эпоха 1, Шаг 5901, loss=4.4079\n",
      "Эпоха 1, Шаг 5951, loss=4.2615\n",
      "Эпоха 1, Шаг 6001, loss=4.3475\n",
      "Эпоха 1, Шаг 6051, loss=4.4178\n",
      "Эпоха 1, Шаг 6101, loss=4.5926\n",
      "Эпоха 1, Шаг 6151, loss=4.4139\n",
      "Эпоха 1, Шаг 6201, loss=4.3941\n",
      "Эпоха 1, Шаг 6251, loss=4.3370\n",
      "Эпоха 1, Шаг 6301, loss=4.3571\n",
      "Эпоха 1, Шаг 6351, loss=4.3179\n",
      "Эпоха 1, Шаг 6401, loss=4.3016\n",
      "Эпоха 1, Шаг 6451, loss=4.3706\n",
      "Эпоха 1, Шаг 6501, loss=4.3023\n",
      "Эпоха 1, Шаг 6551, loss=4.3728\n",
      "Эпоха 1, Шаг 6601, loss=4.3662\n",
      "Эпоха 1, Шаг 6651, loss=4.4618\n",
      "Эпоха 1, Шаг 6701, loss=4.3871\n",
      "Эпоха 1, Шаг 6751, loss=4.3868\n",
      "Эпоха 1, Шаг 6801, loss=4.3171\n",
      "Эпоха 1, Шаг 6851, loss=4.4327\n",
      "Эпоха 1, Шаг 6901, loss=4.2205\n",
      "Эпоха 1, Шаг 6951, loss=4.3143\n",
      "Эпоха 1, Шаг 7001, loss=4.3328\n",
      "Эпоха 1, Шаг 7051, loss=4.2977\n",
      "Эпоха 1, Шаг 7101, loss=4.3926\n",
      "Эпоха 1, Шаг 7151, loss=4.4127\n",
      "Эпоха 1, Шаг 7201, loss=4.2266\n",
      "Эпоха 1, Шаг 7251, loss=4.2071\n",
      "Эпоха 1, Шаг 7301, loss=4.3551\n",
      "Эпоха 1, Шаг 7351, loss=4.2610\n",
      "Эпоха 1, Шаг 7401, loss=4.2464\n",
      "Эпоха 1, Шаг 7451, loss=4.3146\n",
      "Эпоха 1, Шаг 7501, loss=4.3048\n",
      "Эпоха 1, Шаг 7551, loss=4.1658\n",
      "Эпоха 1, Шаг 7601, loss=4.2861\n",
      "Эпоха 1, Шаг 7651, loss=4.3681\n",
      "Эпоха 1, Шаг 7701, loss=4.2172\n",
      "Эпоха 1, Шаг 7751, loss=4.3353\n",
      "Эпоха 1, Шаг 7801, loss=4.2981\n",
      "Эпоха 1, Шаг 7851, loss=4.1895\n",
      "Эпоха 1, Шаг 7901, loss=4.2952\n",
      "Эпоха 1, Шаг 7951, loss=4.2497\n",
      "Эпоха 1, Шаг 8001, loss=4.2545\n",
      "Эпоха 1, Шаг 8051, loss=4.2373\n",
      "Эпоха 1, Шаг 8101, loss=4.1943\n",
      "Эпоха 1, Шаг 8151, loss=4.1349\n",
      "Эпоха 1, Шаг 8201, loss=4.2947\n",
      "Эпоха 1, Шаг 8251, loss=4.2313\n",
      "Эпоха 1, Шаг 8301, loss=4.3162\n",
      "Эпоха 1, Шаг 8351, loss=4.2250\n",
      "Эпоха 1, Шаг 8401, loss=4.0984\n",
      "Эпоха 1, Шаг 8451, loss=4.2007\n",
      "Эпоха 1, Шаг 8501, loss=4.1898\n",
      "Эпоха 1, Шаг 8551, loss=4.1239\n",
      "Эпоха 1, Шаг 8601, loss=4.2300\n",
      "Эпоха 1, Шаг 8651, loss=4.1805\n",
      "Эпоха 1, Шаг 8701, loss=4.1469\n",
      "Эпоха 1, Шаг 8751, loss=4.2082\n",
      "Эпоха 1, Шаг 8801, loss=4.1240\n",
      "Эпоха 1, Шаг 8851, loss=4.1928\n",
      "Эпоха 1, Шаг 8901, loss=4.0947\n",
      "Эпоха 1, Шаг 8951, loss=4.1902\n",
      "Эпоха 1, Шаг 9001, loss=4.1906\n",
      "Эпоха 1, Шаг 9051, loss=4.2309\n",
      "Эпоха 1, Шаг 9101, loss=4.1367\n",
      "Эпоха 1, Шаг 9151, loss=4.1564\n",
      "Эпоха 1, Шаг 9201, loss=4.2214\n",
      "Эпоха 1, Шаг 9251, loss=4.1857\n",
      "Эпоха 1, Шаг 9301, loss=4.1203\n",
      "Эпоха 1, Шаг 9351, loss=4.1120\n",
      "Эпоха 1, Шаг 9401, loss=4.1181\n",
      "Эпоха 1, Шаг 9451, loss=4.0202\n",
      "Эпоха 1, Шаг 9501, loss=4.1774\n",
      "Эпоха 1, Шаг 9551, loss=4.1782\n",
      "Эпоха 1, Шаг 9601, loss=3.9965\n",
      "Эпоха 1, Шаг 9651, loss=4.2227\n",
      "Эпоха 1, Шаг 9701, loss=4.2002\n",
      "Эпоха 1, Шаг 9751, loss=4.0557\n",
      "Эпоха 1, Шаг 9801, loss=4.1084\n",
      "Эпоха 1, Шаг 9851, loss=4.1305\n",
      "Эпоха 1, Шаг 9901, loss=4.1534\n",
      "Эпоха 1, Шаг 9951, loss=4.1964\n",
      "Эпоха 1, Шаг 10001, loss=4.0002\n",
      "Эпоха 1, Шаг 10051, loss=4.1593\n",
      "Эпоха 1, Шаг 10101, loss=4.1470\n",
      "Эпоха 1, Шаг 10151, loss=4.0732\n",
      "Эпоха 1, Шаг 10201, loss=4.0812\n",
      "Эпоха 1, Шаг 10251, loss=4.1043\n",
      "Эпоха 1, Шаг 10301, loss=4.2885\n",
      "Эпоха 1, Шаг 10351, loss=4.0841\n",
      "Эпоха 1, Шаг 10401, loss=3.9919\n",
      "Эпоха 1, Шаг 10451, loss=4.0470\n",
      "Эпоха 1, Шаг 10501, loss=4.1426\n",
      "Эпоха 1, Шаг 10551, loss=4.1700\n",
      "Эпоха 1, Шаг 10601, loss=4.0233\n",
      "Эпоха 1, Шаг 10651, loss=4.0000\n",
      "Эпоха 1, Шаг 10701, loss=4.0338\n",
      "Эпоха 1, Шаг 10751, loss=4.1997\n",
      "Эпоха 1, Шаг 10801, loss=4.0629\n",
      "Эпоха 1, Шаг 10851, loss=3.9575\n",
      "Эпоха 1, Шаг 10901, loss=3.9991\n",
      "Эпоха 1, Шаг 10951, loss=4.2000\n",
      "Эпоха 1, Шаг 11001, loss=4.0522\n",
      "Эпоха 1, Шаг 11051, loss=4.0196\n",
      "Эпоха 1, Шаг 11101, loss=3.9771\n",
      "Эпоха 1, Шаг 11151, loss=4.0524\n",
      "Эпоха 1, Шаг 11201, loss=4.0823\n",
      "Эпоха 1, Шаг 11251, loss=4.0386\n",
      "Эпоха 1, Шаг 11301, loss=4.0394\n",
      "Эпоха 1, Шаг 11351, loss=4.0370\n",
      "Эпоха 1, Шаг 11401, loss=4.1539\n",
      "Эпоха 1, Шаг 11451, loss=3.9269\n",
      "Эпоха 1, Шаг 11501, loss=4.0609\n",
      "Эпоха 1, Шаг 11551, loss=4.0555\n",
      "Эпоха 1, Шаг 11601, loss=4.0621\n",
      "Эпоха 1, Шаг 11651, loss=4.0712\n",
      "Эпоха 1, Шаг 11701, loss=4.0682\n",
      "Эпоха 1, Шаг 11751, loss=3.9572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Эпоха 1, Шаг 11801, loss=4.0259\n",
      "Эпоха 1, Шаг 11851, loss=4.0647\n",
      "Эпоха 1, Шаг 11901, loss=3.9115\n",
      "Эпоха 1, Шаг 11951, loss=3.9339\n",
      "Эпоха 1, Шаг 12001, loss=3.9278\n",
      "Эпоха 1, Шаг 12051, loss=4.0181\n",
      "Эпоха 1, Шаг 12101, loss=4.0862\n",
      "Эпоха 1, Шаг 12151, loss=4.1148\n",
      "Эпоха 1, Шаг 12201, loss=3.8611\n",
      "Эпоха 1, Шаг 12251, loss=4.0715\n",
      "Эпоха 1, Шаг 12301, loss=4.0168\n",
      "Эпоха 1, Шаг 12351, loss=4.0339\n",
      "Эпоха 1, Шаг 12401, loss=4.0555\n",
      "Эпоха 1, Шаг 12451, loss=4.0191\n",
      "Эпоха 1, Шаг 12501, loss=4.0341\n",
      "Эпоха 1, Шаг 12551, loss=4.0244\n",
      "Эпоха 1, Шаг 12601, loss=3.9497\n",
      "Эпоха 1, Шаг 12651, loss=3.9182\n",
      "Эпоха 1, Шаг 12701, loss=4.1380\n",
      "Эпоха 1, Шаг 12751, loss=4.0577\n",
      "Эпоха 1, Шаг 12801, loss=4.1154\n",
      "Эпоха 1, Шаг 12851, loss=4.0636\n",
      "Эпоха 1, Шаг 12901, loss=3.9303\n",
      "Эпоха 1, Шаг 12951, loss=3.8926\n",
      "Эпоха 1, Шаг 13001, loss=4.0658\n",
      "Эпоха 1, Шаг 13051, loss=3.9986\n",
      "Эпоха 1, Шаг 13101, loss=4.0339\n",
      "Эпоха 1, Шаг 13151, loss=4.0371\n",
      "Эпоха 1, Шаг 13201, loss=4.0073\n",
      "Эпоха 1, Шаг 13251, loss=3.9747\n",
      "Эпоха 1, Шаг 13301, loss=3.9263\n",
      "Эпоха 1, Шаг 13351, loss=4.0547\n",
      "Эпоха 1, Шаг 13401, loss=3.9137\n",
      "Эпоха 1, Шаг 13451, loss=4.0125\n",
      "Эпоха 1, Шаг 13501, loss=4.1085\n",
      "Эпоха 1, Шаг 13551, loss=3.9898\n",
      "Эпоха 1, Шаг 13601, loss=4.0026\n",
      "Эпоха 1, Шаг 13651, loss=3.9413\n",
      "Эпоха 1, Шаг 13701, loss=4.0323\n",
      "Эпоха 1, Шаг 13751, loss=3.9416\n",
      "Эпоха 1, Шаг 13801, loss=3.9227\n",
      "Эпоха 1, Шаг 13851, loss=3.8992\n",
      "Эпоха 1, Шаг 13901, loss=4.0260\n",
      "Эпоха 1, Шаг 13951, loss=3.9810\n",
      "Эпоха 1, Шаг 14001, loss=3.9529\n",
      "Эпоха 1, Шаг 14051, loss=3.8683\n",
      "Эпоха 1, Шаг 14101, loss=3.9874\n",
      "Эпоха 1, Шаг 14151, loss=3.9783\n",
      "Эпоха 1, Шаг 14201, loss=3.9842\n",
      "Эпоха 1, Шаг 14251, loss=3.8610\n",
      "Эпоха 1, Шаг 14301, loss=3.8663\n",
      "Эпоха 1, Шаг 14351, loss=3.9749\n",
      "Эпоха 1, Шаг 14401, loss=3.9327\n",
      "Эпоха 1, Шаг 14451, loss=3.9213\n",
      "Эпоха 1, Шаг 14501, loss=4.0704\n",
      "Эпоха 1, Шаг 14551, loss=3.7897\n",
      "Эпоха 1, Шаг 14601, loss=3.9177\n",
      "Эпоха 1, Шаг 14651, loss=3.9283\n",
      "Эпоха 1, Шаг 14701, loss=3.8756\n",
      "Эпоха 1, Шаг 14751, loss=3.9035\n",
      "Эпоха 1, Шаг 14801, loss=3.9332\n",
      "Эпоха 1, Шаг 14851, loss=4.0252\n",
      "Эпоха 1, Шаг 14901, loss=3.9246\n",
      "Эпоха 1, Шаг 14951, loss=3.9201\n",
      "Эпоха 1, Шаг 15001, loss=3.9278\n",
      "Эпоха 1, Шаг 15051, loss=3.9772\n",
      "Эпоха 1, Шаг 15101, loss=3.7711\n",
      "Эпоха 1, Шаг 15151, loss=3.9678\n",
      "Эпоха 1, Шаг 15201, loss=3.9106\n",
      "Эпоха 1, Шаг 15251, loss=3.9813\n",
      "Эпоха 1, Шаг 15301, loss=3.8258\n",
      "Эпоха 1, Шаг 15351, loss=4.0588\n",
      "Эпоха 1, Шаг 15401, loss=3.9998\n",
      "Эпоха 1, Шаг 15451, loss=3.9381\n",
      "Эпоха 1, Шаг 15501, loss=3.8700\n",
      "Эпоха 1, Шаг 15551, loss=3.8611\n",
      "Эпоха 1, Шаг 15601, loss=3.8938\n",
      "Эпоха 1, Шаг 15651, loss=3.8577\n",
      "Эпоха 1, Шаг 15701, loss=3.8215\n",
      "Эпоха 1, Шаг 15751, loss=3.7778\n",
      "Эпоха 1, Шаг 15801, loss=4.0637\n",
      "Эпоха 1, Шаг 15851, loss=3.9112\n",
      "Эпоха 1, Шаг 15901, loss=3.8434\n",
      "Эпоха 1, Шаг 15951, loss=3.8546\n",
      "Эпоха 1, Шаг 16001, loss=3.8478\n",
      "Эпоха 1, Шаг 16051, loss=3.9161\n",
      "Эпоха 1, Шаг 16101, loss=3.9738\n",
      "Эпоха 1, Шаг 16151, loss=3.9690\n",
      "Эпоха 1, Шаг 16201, loss=3.8868\n",
      "Эпоха 1, Шаг 16251, loss=3.8378\n",
      "Эпоха 1, Шаг 16301, loss=3.8627\n",
      "Эпоха 1, Шаг 16351, loss=3.9534\n",
      "Эпоха 1, Шаг 16401, loss=3.9307\n",
      "Эпоха 1, Шаг 16451, loss=3.9069\n",
      "Эпоха 1, Шаг 16501, loss=3.8620\n",
      "Эпоха 1, Шаг 16551, loss=3.8000\n",
      "Эпоха 1, Шаг 16601, loss=3.8282\n",
      "Эпоха 1, Шаг 16651, loss=3.8787\n",
      "Эпоха 1, Шаг 16701, loss=3.8197\n",
      "Эпоха 1, Шаг 16751, loss=3.9371\n",
      "Эпоха 1, Шаг 16801, loss=3.8642\n",
      "Эпоха 1, Шаг 16851, loss=3.9270\n",
      "Эпоха 1, Шаг 16901, loss=3.8707\n",
      "Эпоха 1, Шаг 16951, loss=3.8900\n",
      "Эпоха 1, Шаг 17001, loss=3.8977\n",
      "Эпоха 1, Шаг 17051, loss=3.9612\n",
      "Эпоха 1, Шаг 17101, loss=3.8282\n",
      "Эпоха 1, Шаг 17151, loss=3.8129\n",
      "Эпоха 1, Шаг 17201, loss=3.9740\n",
      "Эпоха 1, Шаг 17251, loss=3.7612\n",
      "Эпоха 1, Шаг 17301, loss=3.9070\n",
      "Эпоха 1, Шаг 17351, loss=3.7853\n",
      "Эпоха 1, Шаг 17401, loss=3.9446\n",
      "Эпоха 1, Шаг 17451, loss=3.8594\n",
      "Эпоха 1, Шаг 17501, loss=3.9253\n",
      "Эпоха 1, Шаг 17551, loss=3.9449\n",
      "Эпоха 1, Шаг 17601, loss=3.9058\n",
      "Эпоха 1, Шаг 17651, loss=3.7880\n",
      "Эпоха 1, Шаг 17701, loss=4.0004\n",
      "Эпоха 1, Шаг 17751, loss=3.8336\n",
      "Эпоха 1, Шаг 17801, loss=3.9208\n",
      "Эпоха 1, Шаг 17851, loss=3.9778\n",
      "Эпоха 1, Шаг 17901, loss=3.8505\n",
      "Эпоха 1, Шаг 17951, loss=3.9080\n",
      "Эпоха 1, Шаг 18001, loss=3.9974\n",
      "Эпоха 1, Шаг 18051, loss=4.0025\n",
      "Эпоха 1, Шаг 18101, loss=3.7691\n",
      "Эпоха 1, Шаг 18151, loss=3.7939\n",
      "Эпоха 1, Шаг 18201, loss=3.7784\n",
      "Эпоха 1, Шаг 18251, loss=3.8387\n",
      "Эпоха 1, Шаг 18301, loss=3.6449\n",
      "Эпоха 1, Шаг 18351, loss=3.8412\n",
      "Эпоха 1, Шаг 18401, loss=3.7469\n",
      "Эпоха 1, Шаг 18451, loss=3.7315\n",
      "Эпоха 1, Шаг 18501, loss=3.8754\n",
      "Эпоха 1, Шаг 18551, loss=3.7949\n",
      "Эпоха 1, Шаг 18601, loss=3.9199\n",
      "Эпоха 1, Шаг 18651, loss=3.7946\n",
      "Эпоха 1, Шаг 18701, loss=3.7390\n",
      "Эпоха 1, Шаг 18751, loss=3.7688\n",
      "Эпоха 1, Шаг 18801, loss=3.7575\n",
      "Эпоха 1, Шаг 18851, loss=3.8412\n",
      "Эпоха 1, Шаг 18901, loss=3.7471\n",
      "Эпоха 1, Шаг 18951, loss=3.8485\n",
      "Эпоха 1, Шаг 19001, loss=3.8631\n",
      "Эпоха 1, Шаг 19051, loss=3.7476\n",
      "Эпоха 1, Шаг 19101, loss=3.8531\n",
      "Эпоха 1, Шаг 19151, loss=3.8084\n",
      "Эпоха 1, Шаг 19201, loss=3.8060\n",
      "Эпоха 1, Шаг 19251, loss=3.8103\n",
      "Эпоха 1, Шаг 19301, loss=3.7242\n",
      "Эпоха 1, Шаг 19351, loss=3.7974\n",
      "Эпоха 1, Шаг 19401, loss=3.8156\n",
      "Эпоха 1, Шаг 19451, loss=3.8986\n",
      "Эпоха 1, Шаг 19501, loss=3.7114\n",
      "Эпоха 1, Шаг 19551, loss=3.7655\n",
      "Эпоха 1, Шаг 19601, loss=3.8236\n",
      "Эпоха 1, Шаг 19651, loss=3.7305\n",
      "Эпоха 1, Шаг 19701, loss=3.8314\n",
      "Эпоха 1, Шаг 19751, loss=3.8449\n",
      "Эпоха 1, Шаг 19801, loss=3.7995\n",
      "Эпоха 1, Шаг 19851, loss=3.7323\n",
      "Эпоха 1, Шаг 19901, loss=3.8061\n",
      "Эпоха 1, Шаг 19951, loss=3.6884\n",
      "Эпоха 1, Шаг 20001, loss=3.7785\n",
      "Эпоха 1, Шаг 20051, loss=3.6204\n",
      "Эпоха 1, Шаг 20101, loss=3.7330\n",
      "Эпоха 1, Шаг 20151, loss=3.8984\n",
      "Эпоха 1, Шаг 20201, loss=3.7303\n",
      "Эпоха 1, Шаг 20251, loss=3.7844\n",
      "Эпоха 1, Шаг 20301, loss=3.7116\n",
      "Эпоха 1, Шаг 20351, loss=3.9091\n",
      "Эпоха 1, Шаг 20401, loss=3.7930\n",
      "Эпоха 1, Шаг 20451, loss=3.8729\n",
      "Эпоха 1, Шаг 20501, loss=3.8527\n",
      "Эпоха 1, Шаг 20551, loss=3.9033\n",
      "Эпоха 1, Шаг 20601, loss=3.9044\n",
      "Эпоха 1, Шаг 20651, loss=3.7879\n",
      "Эпоха 1, Шаг 20701, loss=3.7142\n",
      "Эпоха 1, Шаг 20751, loss=3.8890\n",
      "Эпоха 1, Шаг 20801, loss=3.8115\n",
      "Эпоха 1, Шаг 20851, loss=3.6482\n",
      "Эпоха 1, Шаг 20901, loss=3.7907\n",
      "Эпоха 1, Шаг 20951, loss=3.8428\n",
      "Эпоха 1, Шаг 21001, loss=3.5875\n",
      "Эпоха 1, Шаг 21051, loss=3.7376\n",
      "Эпоха 1, Шаг 21101, loss=3.6621\n",
      "Эпоха 1, Шаг 21151, loss=3.7479\n",
      "Эпоха 1, Шаг 21201, loss=3.7128\n",
      "Эпоха 1, Шаг 21251, loss=3.7368\n",
      "Эпоха 1, Шаг 21301, loss=3.7360\n",
      "Эпоха 1, Шаг 21351, loss=3.6366\n",
      "Эпоха 1, Шаг 21401, loss=3.7997\n",
      "Эпоха 1, Шаг 21451, loss=3.8130\n",
      "Эпоха 1, Шаг 21501, loss=3.7715\n",
      "Эпоха 1, Шаг 21551, loss=3.7079\n",
      "Эпоха 1, Шаг 21601, loss=3.7830\n",
      "Эпоха 1, Шаг 21651, loss=3.6673\n",
      "Эпоха 1, Шаг 21701, loss=3.7097\n",
      "Эпоха 1, Шаг 21751, loss=3.8489\n",
      "Эпоха 1, Шаг 21801, loss=3.7110\n",
      "Эпоха 1, Шаг 21851, loss=3.7701\n",
      "Эпоха 1, Шаг 21901, loss=3.7864\n",
      "Эпоха 1, Шаг 21951, loss=3.8311\n",
      "Эпоха 1, Шаг 22001, loss=3.7928\n",
      "Эпоха 1, Шаг 22051, loss=3.8305\n",
      "Эпоха 1, Шаг 22101, loss=3.6904\n",
      "Эпоха 1, Шаг 22151, loss=3.6206\n",
      "Эпоха 1, Шаг 22201, loss=3.6807\n",
      "Эпоха 1, Шаг 22251, loss=3.7642\n",
      "Эпоха 1, Шаг 22301, loss=3.7414\n",
      "Эпоха 1, Шаг 22351, loss=3.7148\n",
      "Эпоха 1, Шаг 22401, loss=3.7371\n",
      "Эпоха 1, Шаг 22451, loss=3.7460\n",
      "Эпоха 1, Шаг 22501, loss=3.7915\n",
      "Эпоха 1, Шаг 22551, loss=3.6485\n",
      "Эпоха 1, Шаг 22601, loss=3.7828\n",
      "Эпоха 1, Шаг 22651, loss=3.6247\n",
      "Эпоха 1, Шаг 22701, loss=3.7485\n",
      "Эпоха 1, Шаг 22751, loss=3.8384\n",
      "Эпоха 1, Шаг 22801, loss=3.7546\n",
      "Эпоха 1, Шаг 22851, loss=3.5972\n",
      "Эпоха 1, Шаг 22901, loss=3.7488\n",
      "Эпоха 1, Шаг 22951, loss=3.7359\n",
      "Эпоха 1, Шаг 23001, loss=3.6559\n",
      "Эпоха 1, Шаг 23051, loss=3.7904\n",
      "Эпоха 1, Шаг 23101, loss=3.6895\n",
      "Эпоха 1, Шаг 23151, loss=3.5546\n",
      "Эпоха 1, Шаг 23201, loss=3.7561\n",
      "Эпоха 1, Шаг 23251, loss=3.8016\n",
      "Эпоха 1, Шаг 23301, loss=3.6755\n",
      "Эпоха 1, Шаг 23351, loss=3.6327\n",
      "Эпоха 1, Шаг 23401, loss=3.7831\n",
      "Эпоха 1, Шаг 23451, loss=3.5968\n",
      "Эпоха 1, Шаг 23501, loss=3.6640\n",
      "Эпоха 1, Шаг 23551, loss=3.7221\n",
      "Эпоха 1, Шаг 23601, loss=3.8450\n",
      "Эпоха 1, Шаг 23651, loss=3.6881\n",
      "Эпоха 1, Шаг 23701, loss=3.6644\n",
      "Эпоха 1, Шаг 23751, loss=3.6239\n",
      "Эпоха 1, Шаг 23801, loss=3.8099\n",
      "Эпоха 1, Шаг 23851, loss=3.6072\n",
      "Эпоха 1, Шаг 23901, loss=3.7829\n",
      "Эпоха 1, Шаг 23951, loss=3.6599\n",
      "Эпоха 1, Шаг 24001, loss=3.7472\n",
      "Эпоха 1, Шаг 24051, loss=3.7281\n",
      "Эпоха 1, Шаг 24101, loss=3.8290\n",
      "Эпоха 1, Шаг 24151, loss=3.6626\n",
      "Эпоха 1, Шаг 24201, loss=3.7818\n",
      "Эпоха 1, Шаг 24251, loss=3.8057\n",
      "Эпоха 1, Шаг 24301, loss=3.7497\n",
      "Эпоха 1, Шаг 24351, loss=3.5933\n",
      "Эпоха 1, Шаг 24401, loss=3.7116\n",
      "Эпоха 1, Шаг 24451, loss=3.6660\n",
      "Эпоха 1, Шаг 24501, loss=3.6228\n",
      "Эпоха 1, Шаг 24551, loss=3.7041\n",
      "Эпоха 1, Шаг 24601, loss=3.8375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Эпоха 1, Шаг 24651, loss=3.8484\n",
      "Эпоха 1, Шаг 24701, loss=3.6163\n",
      "Эпоха 1, Шаг 24751, loss=3.9371\n",
      "Эпоха 1, Шаг 24801, loss=3.6938\n",
      "Эпоха 1, Шаг 24851, loss=3.7905\n",
      "Эпоха 1, Шаг 24901, loss=3.7776\n",
      "Эпоха 1, Шаг 24951, loss=3.7328\n",
      "Эпоха 1, Шаг 25001, loss=3.7687\n",
      "Эпоха 1, Шаг 25051, loss=3.7057\n",
      "Эпоха 1, Шаг 25101, loss=3.6922\n",
      "Эпоха 1, Шаг 25151, loss=3.7099\n",
      "Эпоха 1, Шаг 25201, loss=3.7999\n",
      "Эпоха 1, Шаг 25251, loss=3.7203\n",
      "Эпоха 1, Шаг 25301, loss=3.7298\n",
      "Эпоха 1, Шаг 25351, loss=3.7027\n",
      "Эпоха 1, Шаг 25401, loss=3.7367\n",
      "Эпоха 1, Шаг 25451, loss=3.6931\n",
      "Эпоха 1, Шаг 25501, loss=3.6705\n",
      "Эпоха 1, Шаг 25551, loss=3.6708\n",
      "Эпоха 1, Шаг 25601, loss=3.8278\n",
      "Эпоха 1, Шаг 25651, loss=3.7006\n",
      "Эпоха 1, Шаг 25701, loss=3.7033\n",
      "Эпоха 1, Шаг 25751, loss=3.6070\n",
      "Эпоха 1, Шаг 25801, loss=3.6250\n",
      "Эпоха 1, Шаг 25851, loss=3.7306\n",
      "Эпоха 1, Шаг 25901, loss=3.7647\n",
      "Эпоха 1, Шаг 25951, loss=3.6836\n",
      "Эпоха 1, Шаг 26001, loss=3.8055\n",
      "Эпоха 1, Шаг 26051, loss=3.7896\n",
      "Эпоха 1, Шаг 26101, loss=3.5309\n",
      "Эпоха 1, Шаг 26151, loss=3.8276\n",
      "Эпоха 1, Шаг 26201, loss=3.7330\n",
      "Эпоха 1, Шаг 26251, loss=3.8108\n",
      "Эпоха 1, Шаг 26301, loss=3.6955\n",
      "Эпоха 1, Шаг 26351, loss=3.6684\n",
      "Эпоха 1, Шаг 26401, loss=3.5944\n",
      "Эпоха 1, Шаг 26451, loss=3.6639\n",
      "Эпоха 1, Шаг 26501, loss=3.7131\n",
      "Эпоха 1, Шаг 26551, loss=3.5776\n",
      "Эпоха 1, Шаг 26601, loss=3.6962\n",
      "Эпоха 1, Шаг 26651, loss=3.7345\n",
      "Эпоха 1, Шаг 26701, loss=3.7510\n",
      "Эпоха 1, Шаг 26751, loss=3.5713\n",
      "Эпоха 1, Шаг 26801, loss=3.7482\n",
      "Эпоха 1, Шаг 26851, loss=3.7460\n",
      "Эпоха 1, Шаг 26901, loss=3.6793\n",
      "Эпоха 1, Шаг 26951, loss=3.6690\n",
      "Эпоха 1, Шаг 27001, loss=3.7480\n",
      "Эпоха 1, Шаг 27051, loss=3.7479\n",
      "Эпоха 1, Шаг 27101, loss=3.7713\n",
      "Эпоха 1, Шаг 27151, loss=3.7935\n",
      "Эпоха 1, Шаг 27201, loss=3.6040\n",
      "Эпоха 1, Шаг 27251, loss=3.6352\n",
      "Эпоха 1, Шаг 27301, loss=3.6855\n",
      "Эпоха 1, Шаг 27351, loss=3.7197\n",
      "Эпоха 1, Шаг 27401, loss=3.7573\n",
      "Эпоха 1, Шаг 27451, loss=3.7839\n",
      "Эпоха 1, Шаг 27501, loss=3.7731\n",
      "Эпоха 1, Шаг 27551, loss=3.5889\n",
      "Эпоха 1, Шаг 27601, loss=3.7259\n",
      "Эпоха 1, Шаг 27651, loss=3.6561\n",
      "Эпоха 1, Шаг 27701, loss=3.6778\n",
      "Эпоха 1, Шаг 27751, loss=3.6632\n",
      "Эпоха 1, Шаг 27801, loss=3.7742\n",
      "Эпоха 1, Шаг 27851, loss=3.6762\n",
      "Эпоха 1, Шаг 27901, loss=3.6726\n",
      "Эпоха 1, Шаг 27951, loss=3.6963\n",
      "Эпоха 1, Шаг 28001, loss=3.6934\n",
      "Эпоха 1, Шаг 28051, loss=3.5784\n",
      "Эпоха 1, Шаг 28101, loss=3.5991\n",
      "Эпоха 1, Шаг 28151, loss=3.6763\n",
      "Эпоха 1, Шаг 28201, loss=3.6173\n",
      "Эпоха 1, Шаг 28251, loss=3.5865\n",
      "Эпоха 1, Шаг 28301, loss=3.5887\n",
      "Эпоха 1, Шаг 28351, loss=3.6057\n",
      "Эпоха 1, Шаг 28401, loss=3.5716\n",
      "Эпоха 1, Шаг 28451, loss=3.6254\n",
      "Эпоха 1, Шаг 28501, loss=3.5990\n",
      "Эпоха 1, Шаг 28551, loss=3.6399\n",
      "Эпоха 1, Шаг 28601, loss=3.7781\n",
      "Эпоха 1, Шаг 28651, loss=3.7738\n",
      "Эпоха 1, Шаг 28701, loss=3.4999\n",
      "Эпоха 1, Шаг 28751, loss=3.7027\n",
      "Эпоха 1, Шаг 28801, loss=3.6972\n",
      "Эпоха 1, Шаг 28851, loss=3.6495\n",
      "Эпоха 1, Шаг 28901, loss=3.5652\n",
      "Эпоха 1, Шаг 28951, loss=3.6442\n",
      "Эпоха 1, Шаг 29001, loss=3.7002\n",
      "Эпоха 1, Шаг 29051, loss=3.6650\n",
      "Эпоха 1, Шаг 29101, loss=3.6796\n",
      "Эпоха 1, Шаг 29151, loss=3.5991\n",
      "Эпоха 1, Шаг 29201, loss=3.6109\n",
      "Эпоха 1, Шаг 29251, loss=3.5941\n",
      "Эпоха 1, Шаг 29301, loss=3.7079\n",
      "Эпоха 1, Шаг 29351, loss=3.6354\n",
      "Эпоха 1, Шаг 29401, loss=3.6983\n",
      "Эпоха 1, Шаг 29451, loss=3.6282\n",
      "Эпоха 1, Шаг 29501, loss=3.6050\n",
      "Эпоха 1, Шаг 29551, loss=3.7065\n",
      "Эпоха 1, Шаг 29601, loss=3.7142\n",
      "Эпоха 1, Шаг 29651, loss=3.6099\n",
      "Эпоха 1, Шаг 29701, loss=3.6252\n",
      "Эпоха 1, Шаг 29751, loss=3.7027\n",
      "Эпоха 1, Шаг 29801, loss=3.6789\n",
      "Эпоха 1, Шаг 29851, loss=3.7618\n",
      "Эпоха 1, Шаг 29901, loss=3.7362\n",
      "Эпоха 1, Шаг 29951, loss=3.7154\n",
      "Эпоха 1, Шаг 30001, loss=3.4800\n",
      "Эпоха 1, Шаг 30051, loss=3.6466\n",
      "Эпоха 1, Шаг 30101, loss=3.6901\n",
      "Эпоха 1, Шаг 30151, loss=3.6254\n",
      "Эпоха 1, Шаг 30201, loss=3.6941\n",
      "Эпоха 1, Шаг 30251, loss=3.7607\n",
      "Эпоха 1, Шаг 30301, loss=3.6502\n",
      "Эпоха 1, Шаг 30351, loss=3.6064\n",
      "Эпоха 1, Шаг 30401, loss=3.7125\n",
      "Эпоха 1, Шаг 30451, loss=3.6462\n",
      "Эпоха 1, Шаг 30501, loss=3.7101\n",
      "Эпоха 1, Шаг 30551, loss=3.6165\n",
      "Эпоха 1, Шаг 30601, loss=3.6776\n",
      "Эпоха 1, Шаг 30651, loss=3.6585\n",
      "Эпоха 1, Шаг 30701, loss=3.7361\n",
      "Эпоха 1, Шаг 30751, loss=3.7458\n",
      "Эпоха 1, Шаг 30801, loss=3.6407\n",
      "Эпоха 1, Шаг 30851, loss=3.5040\n",
      "Эпоха 1, Шаг 30901, loss=3.6796\n",
      "Эпоха 1, Шаг 30951, loss=3.6623\n",
      "Эпоха 1, Шаг 31001, loss=3.7385\n",
      "Эпоха 1, Шаг 31051, loss=3.5255\n",
      "Эпоха 1, Шаг 31101, loss=3.6222\n",
      "Эпоха 1, Шаг 31151, loss=3.6198\n",
      "Эпоха 1, Шаг 31201, loss=3.5812\n",
      "Эпоха 1, Шаг 31251, loss=3.8117\n",
      "Эпоха 1, Шаг 31301, loss=3.5014\n",
      "Эпоха 1, Шаг 31351, loss=3.7488\n",
      "Эпоха 1, Шаг 31401, loss=3.6441\n",
      "Эпоха 1, Шаг 31451, loss=3.6230\n",
      "Эпоха 1, Шаг 31501, loss=3.6686\n",
      "Эпоха 1, Шаг 31551, loss=3.6661\n",
      "Эпоха 1, Шаг 31601, loss=3.5665\n",
      "Эпоха 1, Шаг 31651, loss=3.6182\n",
      "Эпоха 1, Шаг 31701, loss=3.6098\n",
      "Эпоха 1, Шаг 31751, loss=3.7610\n",
      "Эпоха 1, Шаг 31801, loss=3.7169\n",
      "Эпоха 1, Шаг 31851, loss=3.5939\n",
      "Эпоха 1, Шаг 31901, loss=3.6006\n",
      "Эпоха 1, Шаг 31951, loss=3.4756\n",
      "Эпоха 1, Шаг 32001, loss=3.7700\n",
      "Эпоха 1, Шаг 32051, loss=3.6488\n",
      "Эпоха 1, Шаг 32101, loss=3.5808\n",
      "Эпоха 1, Шаг 32151, loss=3.5949\n",
      "Эпоха 1, Шаг 32201, loss=3.6226\n",
      "Эпоха 1, Шаг 32251, loss=3.6662\n",
      "Эпоха 1, Шаг 32301, loss=3.5625\n",
      "Эпоха 1, Шаг 32351, loss=3.5935\n",
      "Эпоха 1, Шаг 32401, loss=3.5196\n",
      "Эпоха 1, Шаг 32451, loss=3.5563\n",
      "Эпоха 1, Шаг 32501, loss=3.6562\n",
      "Эпоха 1, Шаг 32551, loss=3.5580\n",
      "Эпоха 1, Шаг 32601, loss=3.6994\n",
      "Эпоха 1, Шаг 32651, loss=3.5420\n",
      "Эпоха 1, Шаг 32701, loss=3.6584\n",
      "Эпоха 1, Шаг 32751, loss=3.6204\n",
      "Эпоха 1, Шаг 32801, loss=3.4602\n",
      "Эпоха 1, Шаг 32851, loss=3.7323\n",
      "Эпоха 1, Шаг 32901, loss=3.6629\n",
      "Эпоха 1, Шаг 32951, loss=3.7893\n",
      "Эпоха 1, Шаг 33001, loss=3.6036\n",
      "Эпоха 1, Шаг 33051, loss=3.6678\n",
      "Эпоха 1, Шаг 33101, loss=3.6439\n",
      "Эпоха 1, Шаг 33151, loss=3.5894\n",
      "Эпоха 1, Шаг 33201, loss=3.5281\n",
      "Эпоха 1, Шаг 33251, loss=3.6500\n",
      "Эпоха 1, Шаг 33301, loss=3.6006\n",
      "Эпоха 1, Шаг 33351, loss=3.6169\n",
      "Эпоха 1, Шаг 33401, loss=3.6217\n",
      "Эпоха 1, Шаг 33451, loss=3.5366\n",
      "Эпоха 1, Шаг 33501, loss=3.5299\n",
      "Эпоха 1, Шаг 33551, loss=3.6112\n",
      "Эпоха 1, Шаг 33601, loss=3.5254\n",
      "Эпоха 1, Шаг 33651, loss=3.6076\n",
      "Эпоха 1, Шаг 33701, loss=3.5737\n",
      "Эпоха 1, Шаг 33751, loss=3.6218\n",
      "Эпоха 1, Шаг 33801, loss=3.5668\n",
      "Эпоха 1, Шаг 33851, loss=3.5877\n",
      "Эпоха 1, Шаг 33901, loss=3.7178\n",
      "Эпоха 1, Шаг 33951, loss=3.6188\n",
      "Эпоха 1, Шаг 34001, loss=3.5975\n",
      "Эпоха 1, Шаг 34051, loss=3.5154\n",
      "Эпоха 1, Шаг 34101, loss=3.6297\n",
      "Эпоха 1, Шаг 34151, loss=3.5882\n",
      "Эпоха 1, Шаг 34201, loss=3.4396\n",
      "Эпоха 1, Шаг 34251, loss=3.5425\n",
      "Эпоха 1, Шаг 34301, loss=3.6759\n",
      "Эпоха 1, Шаг 34351, loss=3.4943\n",
      "Эпоха 1, Шаг 34401, loss=3.7396\n",
      "Эпоха 1, Шаг 34451, loss=3.5911\n",
      "Эпоха 1, Шаг 34501, loss=3.6944\n",
      "Эпоха 1, Шаг 34551, loss=3.6264\n",
      "Эпоха 1, Шаг 34601, loss=3.4834\n",
      "Эпоха 1, Шаг 34651, loss=3.7207\n",
      "Эпоха 1, Шаг 34701, loss=3.5578\n",
      "Эпоха 1, Шаг 34751, loss=3.5915\n",
      "Эпоха 1, Шаг 34801, loss=3.6446\n",
      "Эпоха 1, Шаг 34851, loss=3.5644\n",
      "Эпоха 1, Шаг 34901, loss=3.7346\n",
      "Эпоха 1, Шаг 34951, loss=3.5852\n",
      "Эпоха 1, Шаг 35001, loss=3.6678\n",
      "Эпоха 1, Шаг 35051, loss=3.4402\n",
      "Эпоха 1, Шаг 35101, loss=3.5135\n",
      "Эпоха 1, Шаг 35151, loss=3.6456\n",
      "Эпоха 1, Шаг 35201, loss=3.5386\n",
      "Эпоха 1, Шаг 35251, loss=3.5570\n",
      "Эпоха 1, Шаг 35301, loss=3.5451\n",
      "Эпоха 1, Шаг 35351, loss=3.5872\n",
      "Эпоха 1, Шаг 35401, loss=3.4360\n",
      "Эпоха 1, Шаг 35451, loss=3.5196\n",
      "Эпоха 1, Шаг 35501, loss=3.6156\n",
      "Эпоха 1, Шаг 35551, loss=3.6043\n",
      "Эпоха 1, Шаг 35601, loss=3.5227\n",
      "Эпоха 1, Шаг 35651, loss=3.4651\n",
      "Эпоха 1, Шаг 35701, loss=3.5515\n",
      "Эпоха 1, Шаг 35751, loss=3.4827\n",
      "Эпоха 1, Шаг 35801, loss=3.5494\n",
      "Эпоха 1, Шаг 35851, loss=3.6637\n",
      "Эпоха 1, Шаг 35901, loss=3.6174\n",
      "Эпоха 1, Шаг 35951, loss=3.5781\n",
      "Эпоха 1, Шаг 36001, loss=3.7600\n",
      "Эпоха 1, Шаг 36051, loss=3.6218\n",
      "Эпоха 1, Шаг 36101, loss=3.5311\n",
      "Эпоха 1, Шаг 36151, loss=3.6738\n",
      "Эпоха 1, Шаг 36201, loss=3.5453\n",
      "Эпоха 1, Шаг 36251, loss=3.6339\n",
      "Эпоха 1, Шаг 36301, loss=3.5228\n",
      "Эпоха 1, Шаг 36351, loss=3.3998\n",
      "Эпоха 1, Шаг 36401, loss=3.6278\n",
      "Эпоха 1, Шаг 36451, loss=3.6201\n",
      "Эпоха 1, Шаг 36501, loss=3.4731\n",
      "Эпоха 1, Шаг 36551, loss=3.5587\n",
      "Эпоха 1, Шаг 36601, loss=3.5529\n",
      "Эпоха 1, Шаг 36651, loss=3.4353\n",
      "Эпоха 1, Шаг 36701, loss=3.5698\n",
      "Эпоха 1, Шаг 36751, loss=3.5255\n",
      "Эпоха 1, Шаг 36801, loss=3.5349\n",
      "Эпоха 1, Шаг 36851, loss=3.5274\n",
      "Эпоха 1, Шаг 36901, loss=3.4950\n",
      "Эпоха 1, Шаг 36951, loss=3.5716\n",
      "Эпоха 1, Шаг 37001, loss=3.6415\n",
      "Эпоха 1, Шаг 37051, loss=3.4740\n",
      "Эпоха 1, Шаг 37101, loss=3.5476\n",
      "Эпоха 1, Шаг 37151, loss=3.4683\n",
      "Эпоха 1, Шаг 37201, loss=3.6105\n",
      "Эпоха 1, Шаг 37251, loss=3.4936\n",
      "Эпоха 1, Шаг 37301, loss=3.5273\n",
      "Эпоха 1, Шаг 37351, loss=3.6404\n",
      "Эпоха 1, Шаг 37401, loss=3.6466\n",
      "Эпоха 1, Шаг 37451, loss=3.4438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Эпоха 1, Шаг 37501, loss=3.5568\n",
      "Эпоха 1, Шаг 37551, loss=3.4615\n",
      "Эпоха 1, Шаг 37601, loss=3.6492\n",
      "Эпоха 1, Шаг 37651, loss=3.5316\n",
      "Эпоха 1, Шаг 37701, loss=3.5273\n",
      "Эпоха 1, Шаг 37751, loss=3.4519\n",
      "Эпоха 1, Шаг 37801, loss=3.4665\n",
      "Эпоха 1, Шаг 37851, loss=3.6667\n",
      "Эпоха 1, Шаг 37901, loss=3.5914\n",
      "Эпоха 1, Шаг 37951, loss=3.4319\n",
      "Эпоха 1, Шаг 38001, loss=3.4760\n",
      "Эпоха 1, Шаг 38051, loss=3.4342\n",
      "Эпоха 1, Шаг 38101, loss=3.4301\n",
      "Эпоха 1, Шаг 38151, loss=3.5850\n",
      "Эпоха 1, Шаг 38201, loss=3.5051\n",
      "Эпоха 1, Шаг 38251, loss=3.6092\n",
      "Эпоха 1, Шаг 38301, loss=3.3949\n",
      "Эпоха 1, Шаг 38351, loss=3.5563\n",
      "Эпоха 1, Шаг 38401, loss=3.5643\n",
      "Эпоха 1, Шаг 38451, loss=3.6711\n",
      "Эпоха 1, Шаг 38501, loss=3.6935\n",
      "Эпоха 1, Шаг 38551, loss=3.5738\n",
      "Эпоха 1, Шаг 38601, loss=3.6811\n",
      "Эпоха 1, Шаг 38651, loss=3.6955\n",
      "Эпоха 1, Шаг 38701, loss=3.5544\n",
      "Эпоха 1, Шаг 38751, loss=3.7543\n",
      "Эпоха 1, Шаг 38801, loss=3.6422\n",
      "Эпоха 1, Шаг 38851, loss=3.5483\n",
      "Эпоха 1, Шаг 38901, loss=3.5963\n",
      "Эпоха 1, Шаг 38951, loss=3.6036\n",
      "Эпоха 1, Шаг 39001, loss=3.5954\n",
      "Эпоха 1, Шаг 39051, loss=3.5076\n",
      "Эпоха 1, Шаг 39101, loss=3.4558\n",
      "Эпоха 1, Шаг 39151, loss=3.5327\n",
      "Эпоха 1, Шаг 39201, loss=3.5704\n",
      "Эпоха 1, Шаг 39251, loss=3.5708\n",
      "Эпоха 1, Шаг 39301, loss=3.6079\n",
      "Эпоха 1, Шаг 39351, loss=3.5467\n",
      "Эпоха 1, Шаг 39401, loss=3.5551\n",
      "Эпоха 1, Шаг 39451, loss=3.5327\n",
      "Эпоха 1, Шаг 39501, loss=3.4944\n",
      "Эпоха 1, Шаг 39551, loss=3.4794\n",
      "Эпоха 1, Шаг 39601, loss=3.3880\n",
      "Эпоха 1, Шаг 39651, loss=3.5618\n",
      "Эпоха 1, Шаг 39701, loss=3.5548\n",
      "Эпоха 1, Шаг 39751, loss=3.4878\n",
      "Эпоха 1, Шаг 39801, loss=3.5475\n",
      "Эпоха 1, Шаг 39851, loss=3.4139\n",
      "Эпоха 1, Шаг 39901, loss=3.6001\n",
      "Эпоха 1, Шаг 39951, loss=3.4628\n",
      "Эпоха 1, Шаг 40001, loss=3.6060\n",
      "Эпоха 1, Шаг 40051, loss=3.5437\n",
      "Эпоха 1, Шаг 40101, loss=3.4509\n",
      "Эпоха 1, Шаг 40151, loss=3.5394\n",
      "Эпоха 1, Шаг 40201, loss=3.5329\n",
      "Эпоха 1, Шаг 40251, loss=3.5802\n",
      "Эпоха 1, Шаг 40301, loss=3.5174\n",
      "Эпоха 1, Шаг 40351, loss=3.4930\n",
      "Эпоха 1, Шаг 40401, loss=3.6569\n",
      "Эпоха 1, Шаг 40451, loss=3.4161\n",
      "Эпоха 1, Шаг 40501, loss=3.5904\n",
      "Эпоха 1, Шаг 40551, loss=3.5800\n",
      "Эпоха 1, Шаг 40601, loss=3.5685\n",
      "Эпоха 1, Шаг 40651, loss=3.5648\n",
      "Эпоха 1, Шаг 40701, loss=3.6704\n",
      "Эпоха 1, Шаг 40751, loss=3.5307\n",
      "Эпоха 1, Шаг 40801, loss=3.6619\n",
      "Эпоха 1, Шаг 40851, loss=3.5831\n",
      "Эпоха 1, Шаг 40901, loss=3.6331\n",
      "Эпоха 1, Шаг 40951, loss=3.4508\n",
      "Эпоха 1, Шаг 41001, loss=3.5924\n",
      "Эпоха 1, Шаг 41051, loss=3.5073\n",
      "Эпоха 1, Шаг 41101, loss=3.6331\n",
      "Эпоха 1, Шаг 41151, loss=3.6573\n",
      "Эпоха 1, Шаг 41201, loss=3.4831\n",
      "Эпоха 1, Шаг 41251, loss=3.4873\n",
      "Эпоха 1, Шаг 41301, loss=3.5278\n",
      "Эпоха 1, Шаг 41351, loss=3.5963\n",
      "Эпоха 1, Шаг 41401, loss=3.5087\n",
      "Эпоха 1, Шаг 41451, loss=3.4884\n",
      "Эпоха 1, Шаг 41501, loss=3.4165\n",
      "Эпоха 1, Шаг 41551, loss=3.4963\n",
      "Эпоха 1, Шаг 41601, loss=3.3907\n",
      "Эпоха 1, Шаг 41651, loss=3.6671\n",
      "Эпоха 1, Шаг 41701, loss=3.4323\n",
      "Эпоха 1, Шаг 41751, loss=3.5468\n",
      "Эпоха 1, Шаг 41801, loss=3.4138\n",
      "Эпоха 1, Шаг 41851, loss=3.4850\n",
      "Эпоха 1, Шаг 41901, loss=3.5257\n",
      "Эпоха 1, Шаг 41951, loss=3.6772\n",
      "Эпоха 1, Шаг 42001, loss=3.6183\n",
      "Эпоха 1, Шаг 42051, loss=3.6055\n",
      "Эпоха 1, Шаг 42101, loss=3.5819\n",
      "Эпоха 1, Шаг 42151, loss=3.6021\n",
      "Эпоха 1, Шаг 42201, loss=3.5139\n",
      "Эпоха 1, Шаг 42251, loss=3.4921\n",
      "Эпоха 1, Шаг 42301, loss=3.4809\n",
      "Эпоха 1, Шаг 42351, loss=3.5283\n",
      "Эпоха 1, Шаг 42401, loss=3.5159\n",
      "Эпоха 1, Шаг 42451, loss=3.5602\n",
      "Эпоха 1, Шаг 42501, loss=3.5159\n",
      "Эпоха 1, Шаг 42551, loss=3.6168\n",
      "Эпоха 1, Шаг 42601, loss=3.6366\n",
      "Эпоха 1, Шаг 42651, loss=3.4696\n",
      "Эпоха 1, Шаг 42701, loss=3.5071\n",
      "Эпоха 1, Шаг 42751, loss=3.5837\n",
      "Эпоха 1, Шаг 42801, loss=3.6389\n",
      "Эпоха 1, Шаг 42851, loss=3.4963\n",
      "Эпоха 1, Шаг 42901, loss=3.4822\n",
      "Эпоха 1, Шаг 42951, loss=3.5569\n",
      "Эпоха 1, Шаг 43001, loss=3.5350\n",
      "Эпоха 1, Шаг 43051, loss=3.5130\n",
      "Эпоха 1, Шаг 43101, loss=3.3328\n",
      "Эпоха 1, Шаг 43151, loss=3.6289\n",
      "Эпоха 1, Шаг 43201, loss=3.4495\n",
      "Эпоха 1, Шаг 43251, loss=3.6208\n",
      "Эпоха 1, Шаг 43301, loss=3.5430\n",
      "Эпоха 1, Шаг 43351, loss=3.4878\n",
      "Эпоха 1, Шаг 43401, loss=3.5012\n",
      "Эпоха 1, Шаг 43451, loss=3.5388\n",
      "Эпоха 1, Шаг 43501, loss=3.6414\n",
      "Эпоха 1, Шаг 43551, loss=3.3352\n",
      "Эпоха 1, Шаг 43601, loss=3.6080\n",
      "Эпоха 1, Шаг 43651, loss=3.6016\n",
      "Эпоха 1, Шаг 43701, loss=3.5286\n",
      "Эпоха 1, Шаг 43751, loss=3.5455\n",
      "Эпоха 1, Шаг 43801, loss=3.4955\n",
      "Эпоха 1, Шаг 43851, loss=3.6103\n",
      "Эпоха 1, Шаг 43901, loss=3.4632\n",
      "Эпоха 1, Шаг 43951, loss=3.6942\n",
      "Эпоха 1, Шаг 44001, loss=3.6052\n",
      "Эпоха 1, Шаг 44051, loss=3.5825\n",
      "Эпоха 1, Шаг 44101, loss=3.6538\n",
      "Эпоха 1, Шаг 44151, loss=3.6025\n",
      "Эпоха 1, Шаг 44201, loss=3.4729\n",
      "Эпоха 1, Шаг 44251, loss=3.5575\n",
      "Эпоха 1, Шаг 44301, loss=3.4957\n",
      "Эпоха 1, Шаг 44351, loss=3.5073\n",
      "Эпоха 1, Шаг 44401, loss=3.5486\n",
      "Эпоха 1, Шаг 44451, loss=3.5931\n",
      "Эпоха 1, Шаг 44501, loss=3.4630\n",
      "Эпоха 1, Шаг 44551, loss=3.5231\n",
      "Эпоха 1, Шаг 44601, loss=3.5676\n",
      "Эпоха 1, Шаг 44651, loss=3.5269\n",
      "Эпоха 1, Шаг 44701, loss=3.6417\n",
      "Эпоха 1, Шаг 44751, loss=3.6572\n",
      "Эпоха 1, Шаг 44801, loss=3.6164\n",
      "Эпоха 1, Шаг 44851, loss=3.4719\n",
      "Эпоха 1, Шаг 44901, loss=3.5402\n",
      "Эпоха 1, Шаг 44951, loss=3.4012\n",
      "Эпоха 1, Шаг 45001, loss=3.2782\n",
      "Эпоха 1, Шаг 45051, loss=3.4473\n",
      "Эпоха 1, Шаг 45101, loss=3.5493\n",
      "Эпоха 1, Шаг 45151, loss=3.6374\n",
      "Эпоха 1, Шаг 45201, loss=3.4970\n",
      "Эпоха 1, Шаг 45251, loss=3.4833\n",
      "Эпоха 1, Шаг 45301, loss=3.4909\n",
      "Эпоха 1, Шаг 45351, loss=3.4629\n",
      "Эпоха 1, Шаг 45401, loss=3.6408\n",
      "Эпоха 1, Шаг 45451, loss=3.5659\n",
      "Эпоха 1, Шаг 45501, loss=3.5940\n",
      "Эпоха 1, Шаг 45551, loss=3.3592\n",
      "Эпоха 1, Шаг 45601, loss=3.3721\n",
      "Эпоха 1, Шаг 45651, loss=3.5449\n",
      "Эпоха 1, Шаг 45701, loss=3.4057\n",
      "Эпоха 1, Шаг 45751, loss=3.3747\n",
      "Эпоха 1, Шаг 45801, loss=3.4523\n",
      "Эпоха 1, Шаг 45851, loss=3.5071\n",
      "Эпоха 1, Шаг 45901, loss=3.5189\n",
      "Эпоха 1, Шаг 45951, loss=3.5978\n",
      "Эпоха 1, Шаг 46001, loss=3.4723\n",
      "Эпоха 1, Шаг 46051, loss=3.6208\n",
      "Эпоха 1, Шаг 46101, loss=3.5013\n",
      "Эпоха 1, Шаг 46151, loss=3.4491\n",
      "Эпоха 1, Шаг 46201, loss=3.5270\n",
      "Эпоха 1, Шаг 46251, loss=3.5217\n",
      "Эпоха 1, Шаг 46301, loss=3.4876\n",
      "Эпоха 1, Шаг 46351, loss=3.4742\n",
      "Эпоха 1, Шаг 46401, loss=3.5103\n",
      "Эпоха 1, Шаг 46451, loss=3.5523\n",
      "Эпоха 1, Шаг 46501, loss=3.5159\n",
      "Эпоха 1, Шаг 46551, loss=3.5256\n",
      "Эпоха 1, Шаг 46601, loss=3.6968\n",
      "Эпоха 1, Шаг 46651, loss=3.5880\n",
      "Эпоха 1, Шаг 46701, loss=3.4996\n",
      "Эпоха 1, Шаг 46751, loss=3.6416\n",
      "Эпоха 1, Шаг 46801, loss=3.5999\n",
      "Эпоха 1, Шаг 46851, loss=3.6684\n",
      "Эпоха 1, Шаг 46901, loss=3.5832\n",
      "Эпоха 1, Шаг 46951, loss=3.5696\n",
      "Эпоха 1, Шаг 47001, loss=3.4410\n",
      "Эпоха 1, Шаг 47051, loss=3.5161\n",
      "Эпоха 1, Шаг 47101, loss=3.5273\n",
      "Эпоха 1, Шаг 47151, loss=3.4422\n",
      "Эпоха 1, Шаг 47201, loss=3.5818\n",
      "Эпоха 1, Шаг 47251, loss=3.5887\n",
      "Эпоха 1, Шаг 47301, loss=3.7143\n",
      "Эпоха 1, Шаг 47351, loss=3.3080\n",
      "Эпоха 1, Шаг 47401, loss=3.3433\n",
      "Эпоха 1, Шаг 47451, loss=3.4615\n",
      "Эпоха 1, Шаг 47501, loss=3.4862\n",
      "Эпоха 1, Шаг 47551, loss=3.5129\n",
      "Эпоха 1, Шаг 47601, loss=3.5136\n",
      "Эпоха 1, Шаг 47651, loss=3.4248\n",
      "Эпоха 1, Шаг 47701, loss=3.4321\n",
      "Эпоха 1, Шаг 47751, loss=3.3945\n",
      "Эпоха 1, Шаг 47801, loss=3.5577\n",
      "Эпоха 1, Шаг 47851, loss=3.4547\n",
      "Эпоха 1, Шаг 47901, loss=3.6071\n",
      "Эпоха 1, Шаг 47951, loss=3.4632\n",
      "Эпоха 1, Шаг 48001, loss=3.4121\n",
      "Эпоха 1, Шаг 48051, loss=3.3289\n",
      "Эпоха 1, Шаг 48101, loss=3.5450\n",
      "Эпоха 1, Шаг 48151, loss=3.4410\n",
      "Эпоха 1, Шаг 48201, loss=3.4074\n",
      "Эпоха 1, Шаг 48251, loss=3.5550\n",
      "Эпоха 1, Шаг 48301, loss=3.5117\n",
      "Эпоха 1, Шаг 48351, loss=3.5235\n",
      "Эпоха 1, Шаг 48401, loss=3.3583\n",
      "Эпоха 1, Шаг 48451, loss=3.4775\n",
      "Эпоха 1, Шаг 48501, loss=3.4566\n",
      "Эпоха 1, Шаг 48551, loss=3.4092\n",
      "Эпоха 1, Шаг 48601, loss=3.3960\n",
      "Эпоха 1, Шаг 48651, loss=3.5049\n",
      "Эпоха 1, Шаг 48701, loss=3.5428\n",
      "Эпоха 1, Шаг 48751, loss=3.4395\n",
      "Эпоха 1, Шаг 48801, loss=3.4565\n",
      "Эпоха 1, Шаг 48851, loss=3.4047\n",
      "Эпоха 1, Шаг 48901, loss=3.4344\n",
      "Эпоха 1, Шаг 48951, loss=3.4965\n",
      "Эпоха 1, Шаг 49001, loss=3.5238\n",
      "Эпоха 1, Шаг 49051, loss=3.4544\n",
      "Эпоха 1, Шаг 49101, loss=3.6418\n",
      "Эпоха 1, Шаг 49151, loss=3.4334\n",
      "Эпоха 1, Шаг 49201, loss=3.5589\n",
      "Эпоха 1, Шаг 49251, loss=3.4040\n",
      "Эпоха 1, Шаг 49301, loss=3.6476\n",
      "Эпоха 1, Шаг 49351, loss=3.4301\n",
      "Эпоха 1, Шаг 49401, loss=3.4988\n",
      "Эпоха 1, Шаг 49451, loss=3.4378\n",
      "Эпоха 1, Шаг 49501, loss=3.5166\n",
      "Эпоха 1, Шаг 49551, loss=3.4433\n",
      "Эпоха 1, Шаг 49601, loss=3.5960\n",
      "Эпоха 1, Шаг 49651, loss=3.5129\n",
      "Эпоха 1, Шаг 49701, loss=3.4671\n",
      "Эпоха 1, Шаг 49751, loss=3.3785\n",
      "Эпоха 1, Шаг 49801, loss=3.4267\n",
      "Эпоха 1, Шаг 49851, loss=3.4110\n",
      "Эпоха 1, Шаг 49901, loss=3.5616\n",
      "Эпоха 1, Шаг 49951, loss=3.3820\n",
      "Эпоха 1, Шаг 50001, loss=3.5527\n",
      "Эпоха 1, Шаг 50051, loss=3.5410\n",
      "Эпоха 1, Шаг 50101, loss=3.5065\n",
      "Эпоха 1, Шаг 50151, loss=3.5160\n",
      "Эпоха 1, Шаг 50201, loss=3.4034\n",
      "Эпоха 1, Шаг 50251, loss=3.5738\n",
      "Эпоха 1, Шаг 50301, loss=3.5176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Эпоха 1, Шаг 50351, loss=3.5400\n",
      "Эпоха 1, Шаг 50401, loss=3.5469\n",
      "Эпоха 1, Шаг 50451, loss=3.4418\n",
      "Эпоха 1, Шаг 50501, loss=3.5585\n",
      "Эпоха 1, Шаг 50551, loss=3.4621\n",
      "Эпоха 1, Шаг 50601, loss=3.5537\n",
      "Эпоха 1, Шаг 50651, loss=3.3442\n",
      "Эпоха 1, Шаг 50701, loss=3.3938\n",
      "Эпоха 1, Шаг 50751, loss=3.5074\n",
      "Эпоха 1, Шаг 50801, loss=3.5460\n",
      "Эпоха 1, Шаг 50851, loss=3.5179\n",
      "Эпоха 1, Шаг 50901, loss=3.3576\n",
      "Эпоха 1, Шаг 50951, loss=3.4197\n",
      "Эпоха 1, Шаг 51001, loss=3.1230\n",
      "Эпоха 1, Шаг 51051, loss=3.5734\n",
      "Эпоха 1, Шаг 51101, loss=3.3745\n",
      "Эпоха 1, Шаг 51151, loss=3.4417\n",
      "Эпоха 1, Шаг 51201, loss=3.6316\n",
      "Эпоха 1, Шаг 51251, loss=3.4785\n",
      "Эпоха 1, Шаг 51301, loss=3.4948\n",
      "Эпоха 1, Шаг 51351, loss=3.5148\n",
      "Эпоха 1, Шаг 51401, loss=3.4327\n",
      "Эпоха 1, Шаг 51451, loss=3.4032\n",
      "Эпоха 1, Шаг 51501, loss=3.4615\n",
      "Эпоха 1, Шаг 51551, loss=3.4266\n",
      "Эпоха 1, Шаг 51601, loss=3.5841\n",
      "Эпоха 1, Шаг 51651, loss=3.5108\n",
      "Эпоха 1, Шаг 51701, loss=3.4505\n",
      "Эпоха 1, Шаг 51751, loss=3.3707\n",
      "Эпоха 1, Шаг 51801, loss=3.3707\n",
      "Эпоха 1, Шаг 51851, loss=3.4246\n",
      "Эпоха 1, Шаг 51901, loss=3.5157\n",
      "Эпоха 1, Шаг 51951, loss=3.6189\n",
      "Эпоха 1, Шаг 52001, loss=3.4604\n",
      "Эпоха 1, Шаг 52051, loss=3.3432\n",
      "Эпоха 1, Шаг 52101, loss=3.5503\n",
      "Эпоха 1, Шаг 52151, loss=3.4066\n",
      "Эпоха 1, Шаг 52201, loss=3.5406\n",
      "Эпоха 1, Шаг 52251, loss=3.4835\n",
      "Эпоха 1, Шаг 52301, loss=3.3812\n",
      "Эпоха 1, Шаг 52351, loss=3.5791\n",
      "Эпоха 1, Шаг 52401, loss=3.4453\n",
      "Эпоха 1, Шаг 52451, loss=3.3919\n",
      "Эпоха 1, Шаг 52501, loss=3.5024\n",
      "Эпоха 1, Шаг 52551, loss=3.3508\n",
      "Эпоха 1, Шаг 52601, loss=3.4306\n",
      "Эпоха 1, Шаг 52651, loss=3.5237\n",
      "Эпоха 1, Шаг 52701, loss=3.4511\n",
      "Эпоха 1, Шаг 52751, loss=3.4819\n",
      "Эпоха 1, Шаг 52801, loss=3.4952\n",
      "Эпоха 1, Шаг 52851, loss=3.5897\n",
      "Эпоха 1, Шаг 52901, loss=3.4930\n",
      "Эпоха 1, Шаг 52951, loss=3.3795\n",
      "Эпоха 1, Шаг 53001, loss=3.4423\n",
      "Эпоха 1, Шаг 53051, loss=3.4270\n",
      "Эпоха 1, Шаг 53101, loss=3.4622\n",
      "Эпоха 1, Шаг 53151, loss=3.4188\n",
      "Эпоха 1, Шаг 53201, loss=3.5259\n",
      "Эпоха 1, Шаг 53251, loss=3.5618\n",
      "Эпоха 1, Шаг 53301, loss=3.4109\n",
      "Эпоха 1, Шаг 53351, loss=3.3977\n",
      "Эпоха 1, Шаг 53401, loss=3.5079\n",
      "Эпоха 1, Шаг 53451, loss=3.5912\n",
      "Эпоха 1, Шаг 53501, loss=3.4929\n",
      "Эпоха 1, Шаг 53551, loss=3.3312\n",
      "Эпоха 1, Шаг 53601, loss=3.4321\n",
      "Эпоха 1, Шаг 53651, loss=3.4591\n",
      "Эпоха 1, Шаг 53701, loss=3.4432\n",
      "Эпоха 1, Шаг 53751, loss=3.4306\n",
      "Эпоха 1, Шаг 53801, loss=3.4650\n",
      "Эпоха 1, Шаг 53851, loss=3.3035\n",
      "Эпоха 1, Шаг 53901, loss=3.5561\n",
      "Эпоха 1, Шаг 53951, loss=3.4777\n",
      "Эпоха 1, Шаг 54001, loss=3.5297\n",
      "Эпоха 1, Шаг 54051, loss=3.4976\n",
      "Эпоха 1, Шаг 54101, loss=3.3379\n",
      "Эпоха 1, Шаг 54151, loss=3.4425\n",
      "Эпоха 1, Шаг 54201, loss=3.5232\n",
      "Эпоха 1, Шаг 54251, loss=3.4205\n",
      "Эпоха 1, Шаг 54301, loss=3.3921\n",
      "Эпоха 1, Шаг 54351, loss=3.3821\n",
      "Эпоха 1, Шаг 54401, loss=3.3273\n",
      "Эпоха 1, Шаг 54451, loss=3.5397\n",
      "Эпоха 1, Шаг 54501, loss=3.3956\n",
      "Эпоха 1, Шаг 54551, loss=3.5703\n",
      "Эпоха 1, Шаг 54601, loss=3.4232\n",
      "Эпоха 1, Шаг 54651, loss=3.4369\n",
      "Эпоха 1, Шаг 54701, loss=3.5006\n",
      "Эпоха 1, Шаг 54751, loss=3.5424\n",
      "Эпоха 1, Шаг 54801, loss=3.5529\n",
      "Эпоха 1, Шаг 54851, loss=3.4671\n",
      "Эпоха 1, Шаг 54901, loss=3.3511\n",
      "Эпоха 1, Шаг 54951, loss=3.4361\n",
      "Эпоха 1, Шаг 55001, loss=3.4720\n",
      "Эпоха 1, Шаг 55051, loss=3.4200\n",
      "Эпоха 1, Шаг 55101, loss=3.4978\n",
      "Эпоха 1, Шаг 55151, loss=3.5863\n",
      "Эпоха 1, Шаг 55201, loss=3.3388\n",
      "Эпоха 1, Шаг 55251, loss=3.3781\n",
      "Эпоха 1, Шаг 55301, loss=3.4275\n",
      "Эпоха 1, Шаг 55351, loss=3.5458\n",
      "Эпоха 1, Шаг 55401, loss=3.4090\n",
      "Эпоха 1, Шаг 55451, loss=3.3716\n",
      "Эпоха 1, Шаг 55501, loss=3.6170\n",
      "Эпоха 1, Шаг 55551, loss=3.3629\n",
      "Эпоха 1, Шаг 55601, loss=3.3390\n",
      "Эпоха 1, Шаг 55651, loss=3.5866\n",
      "Эпоха 1, Шаг 55701, loss=3.4632\n",
      "Эпоха 1, Шаг 55751, loss=3.4105\n",
      "Эпоха 1, Шаг 55801, loss=3.4081\n",
      "Эпоха 1, Шаг 55851, loss=3.4332\n",
      "Эпоха 1, Шаг 55901, loss=3.2983\n",
      "Эпоха 1, Шаг 55951, loss=3.3587\n",
      "Эпоха 1, Шаг 56001, loss=3.3633\n",
      "Эпоха 1, Шаг 56051, loss=3.3752\n",
      "Эпоха 1, Шаг 56101, loss=3.4585\n",
      "Эпоха 1, Шаг 56151, loss=3.3598\n",
      "Эпоха 1, Шаг 56201, loss=3.3251\n",
      "Эпоха 1, Шаг 56251, loss=3.3656\n",
      "Эпоха 1, Шаг 56301, loss=3.4904\n",
      "Эпоха 1, Шаг 56351, loss=3.4384\n",
      "Эпоха 1, Шаг 56401, loss=3.4744\n",
      "Эпоха 1, Шаг 56451, loss=3.3311\n",
      "Эпоха 1, Шаг 56501, loss=3.4952\n",
      "Эпоха 1, Шаг 56551, loss=3.4665\n",
      "Эпоха 1, Шаг 56601, loss=3.4737\n",
      "Эпоха 1, Шаг 56651, loss=3.3687\n",
      "Эпоха 1, Шаг 56701, loss=3.5917\n",
      "Эпоха 1, Шаг 56751, loss=3.3279\n",
      "Эпоха 1, Шаг 56801, loss=3.4322\n",
      "Эпоха 1, Шаг 56851, loss=3.5370\n",
      "Эпоха 1, Шаг 56901, loss=3.3306\n",
      "Эпоха 1, Шаг 56951, loss=3.2549\n",
      "Эпоха 1, Шаг 57001, loss=3.4111\n",
      "Эпоха 1, Шаг 57051, loss=3.5055\n",
      "Эпоха 1, Шаг 57101, loss=3.2971\n",
      "Эпоха 1, Шаг 57151, loss=3.3828\n",
      "Эпоха 1, Шаг 57201, loss=3.3922\n",
      "Эпоха 1, Шаг 57251, loss=3.3512\n",
      "Эпоха 1, Шаг 57301, loss=3.3250\n",
      "Эпоха 1, Шаг 57351, loss=3.3823\n",
      "Эпоха 1, Шаг 57401, loss=3.3873\n",
      "Эпоха 1, Шаг 57451, loss=3.4698\n",
      "Эпоха 1, Шаг 57501, loss=3.5079\n",
      "Эпоха 1, Шаг 57551, loss=3.4469\n",
      "Эпоха 1, Шаг 57601, loss=3.3935\n",
      "Эпоха 1, Шаг 57651, loss=3.4299\n",
      "Эпоха 1, Шаг 57701, loss=3.4613\n",
      "Эпоха 1, Шаг 57751, loss=3.5240\n",
      "Эпоха 1, Шаг 57801, loss=3.4792\n",
      "Эпоха 1, Шаг 57851, loss=3.3845\n",
      "Эпоха 1, Шаг 57901, loss=3.3420\n",
      "Эпоха 1, Шаг 57951, loss=3.4205\n",
      "Эпоха 1, Шаг 58001, loss=3.3997\n",
      "Эпоха 1, Шаг 58051, loss=3.3770\n",
      "Эпоха 1, Шаг 58101, loss=3.4024\n",
      "Эпоха 1, Шаг 58151, loss=3.5292\n",
      "Эпоха 1, Шаг 58201, loss=3.3999\n",
      "Эпоха 1, Шаг 58251, loss=3.5287\n",
      "Эпоха 1, Шаг 58301, loss=3.4215\n",
      "Эпоха 1, Шаг 58351, loss=3.5717\n",
      "Эпоха 1, Шаг 58401, loss=3.4791\n",
      "Эпоха 1, Шаг 58451, loss=3.2791\n",
      "Эпоха 1, Шаг 58501, loss=3.4236\n",
      "Эпоха 1, Шаг 58551, loss=3.2892\n",
      "Эпоха 1, Шаг 58601, loss=3.4399\n",
      "Эпоха 1, Шаг 58651, loss=3.4489\n",
      "Эпоха 1, Шаг 58701, loss=3.3812\n",
      "Эпоха 1, Шаг 58751, loss=3.4465\n",
      "Эпоха 1, Шаг 58801, loss=3.5481\n",
      "Эпоха 1, Шаг 58851, loss=3.5586\n",
      "Эпоха 1, Шаг 58901, loss=3.4894\n",
      "Эпоха 1, Шаг 58951, loss=3.4539\n",
      "Эпоха 1, Шаг 59001, loss=3.5311\n",
      "Эпоха 1, Шаг 59051, loss=3.4084\n",
      "Эпоха 1, Шаг 59101, loss=3.4729\n",
      "Эпоха 1, Шаг 59151, loss=3.4003\n",
      "Эпоха 1, Шаг 59201, loss=3.4281\n",
      "Эпоха 1, Шаг 59251, loss=3.4518\n",
      "Эпоха 1, Шаг 59301, loss=3.4736\n",
      "Эпоха 1, Шаг 59351, loss=3.4984\n",
      "Эпоха 1, Шаг 59401, loss=3.4172\n",
      "Эпоха 1, Шаг 59451, loss=3.4618\n",
      "Эпоха 1, Шаг 59501, loss=3.5095\n",
      "Эпоха 1, Шаг 59551, loss=3.5089\n",
      "Эпоха 1, Шаг 59601, loss=3.4856\n",
      "Эпоха 1, Шаг 59651, loss=3.5205\n",
      "Эпоха 1, Шаг 59701, loss=3.3869\n",
      "Эпоха 1, Шаг 59751, loss=3.5024\n",
      "Эпоха 1, Шаг 59801, loss=3.4854\n",
      "Эпоха 1, Шаг 59851, loss=3.5075\n",
      "Эпоха 1, Шаг 59901, loss=3.4821\n",
      "Эпоха 1, Шаг 59951, loss=3.3939\n",
      "Эпоха 1, Шаг 60001, loss=3.4106\n",
      "Эпоха 1, Шаг 60051, loss=3.5580\n",
      "Эпоха 1, Шаг 60101, loss=3.3292\n",
      "Эпоха 1, Шаг 60151, loss=3.4649\n",
      "Эпоха 1, Шаг 60201, loss=3.4117\n",
      "Эпоха 1, Шаг 60251, loss=3.4731\n",
      "Эпоха 1, Шаг 60301, loss=3.3438\n",
      "Эпоха 1, Шаг 60351, loss=3.3836\n",
      "Эпоха 1, Шаг 60401, loss=3.4395\n",
      "Эпоха 1, Шаг 60451, loss=3.3700\n",
      "Эпоха 1, Шаг 60501, loss=3.4704\n",
      "Эпоха 1, Шаг 60551, loss=3.3721\n",
      "Эпоха 1, Шаг 60601, loss=3.4773\n",
      "Эпоха 1, Шаг 60651, loss=3.3794\n",
      "Эпоха 1, Шаг 60701, loss=3.5127\n",
      "Эпоха 1, Шаг 60751, loss=3.3497\n",
      "Эпоха 1, Шаг 60801, loss=3.4311\n",
      "Эпоха 1, Шаг 60851, loss=3.4280\n",
      "Эпоха 1, Шаг 60901, loss=3.4578\n",
      "Эпоха 1, Шаг 60951, loss=3.4469\n",
      "Эпоха 1, Шаг 61001, loss=3.3983\n",
      "Эпоха 1, Шаг 61051, loss=3.4598\n",
      "Эпоха 1, Шаг 61101, loss=3.4692\n",
      "Эпоха 1, Шаг 61151, loss=3.4176\n",
      "Эпоха 1, Шаг 61201, loss=3.3389\n",
      "Эпоха 1, Шаг 61251, loss=3.2154\n",
      "Эпоха 1, Шаг 61301, loss=3.4506\n",
      "Эпоха 1, Шаг 61351, loss=3.4155\n",
      "Эпоха 1, Шаг 61401, loss=3.4049\n",
      "Эпоха 1, Шаг 61451, loss=3.5373\n",
      "Эпоха 1, Шаг 61501, loss=3.5123\n",
      "Эпоха 1, Шаг 61551, loss=3.5222\n",
      "Эпоха 1, Шаг 61601, loss=3.3700\n",
      "Эпоха 1, Шаг 61651, loss=3.4282\n",
      "Эпоха 1, Шаг 61701, loss=3.3407\n",
      "Эпоха 1, Шаг 61751, loss=3.4633\n",
      "Эпоха 1, Шаг 61801, loss=3.4748\n",
      "Эпоха 1, Шаг 61851, loss=3.4551\n",
      "Эпоха 1, Шаг 61901, loss=3.4059\n",
      "Эпоха 1, Шаг 61951, loss=3.5106\n",
      "Эпоха 1, Шаг 62001, loss=3.3451\n",
      "Эпоха 1, Шаг 62051, loss=3.5668\n",
      "Эпоха 1, Шаг 62101, loss=3.3688\n",
      "Эпоха 1, Шаг 62151, loss=3.4231\n",
      "Эпоха 1, Шаг 62201, loss=3.4676\n",
      "Эпоха 1, Шаг 62251, loss=3.5784\n",
      "Эпоха 1, Шаг 62301, loss=3.4035\n",
      "Эпоха 1, Шаг 62351, loss=3.4552\n",
      "Эпоха 1, Шаг 62401, loss=3.3283\n",
      "Эпоха 1, Шаг 62451, loss=3.3558\n",
      "Эпоха 1, Шаг 62501, loss=3.5221\n",
      "Эпоха 1, Шаг 62551, loss=3.3969\n",
      "Эпоха 1, Шаг 62601, loss=3.3348\n",
      "Эпоха 1, Шаг 62651, loss=3.4073\n",
      "Эпоха 1, Шаг 62701, loss=3.3612\n",
      "Эпоха 1, Шаг 62751, loss=3.4417\n",
      "Эпоха 1, Шаг 62801, loss=3.4232\n",
      "Эпоха 1, Шаг 62851, loss=3.5464\n",
      "Эпоха 1, Шаг 62901, loss=3.2372\n",
      "Эпоха 1, Шаг 62951, loss=3.4786\n",
      "Эпоха 1, Шаг 63001, loss=3.4200\n",
      "Эпоха 1, Шаг 63051, loss=3.4875\n",
      "Эпоха 1, Шаг 63101, loss=3.3390\n",
      "Эпоха 1, Шаг 63151, loss=3.4784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Эпоха 1, Шаг 63201, loss=3.3297\n",
      "Эпоха 1, Шаг 63251, loss=3.2950\n",
      "Эпоха 1, Шаг 63301, loss=3.4469\n",
      "Эпоха 1, Шаг 63351, loss=3.4597\n",
      "Эпоха 1, Шаг 63401, loss=3.3033\n",
      "Эпоха 1, Шаг 63451, loss=3.4298\n",
      "Эпоха 1, Шаг 63501, loss=3.4507\n",
      "Эпоха 1, Шаг 63551, loss=3.4074\n",
      "Эпоха 1, Шаг 63601, loss=3.4449\n",
      "Эпоха 1, Шаг 63651, loss=3.4138\n",
      "Эпоха 1, Шаг 63701, loss=3.4286\n",
      "Эпоха 1, Шаг 63751, loss=3.3624\n",
      "Эпоха 1, Шаг 63801, loss=3.2605\n",
      "Эпоха 1, Шаг 63851, loss=3.3419\n",
      "Эпоха 1, Шаг 63901, loss=3.2927\n",
      "Эпоха 1, Шаг 63951, loss=3.2413\n",
      "Эпоха 1, Шаг 64001, loss=3.4113\n",
      "Эпоха 1, Шаг 64051, loss=3.2881\n",
      "Эпоха 1, Шаг 64101, loss=3.2380\n",
      "Эпоха 1, Шаг 64151, loss=3.3473\n",
      "Эпоха 1, Шаг 64201, loss=3.6093\n",
      "Эпоха 1, Шаг 64251, loss=3.4986\n",
      "Эпоха 1, Шаг 64301, loss=3.4780\n",
      "Эпоха 1, Шаг 64351, loss=3.3713\n",
      "Эпоха 1, Шаг 64401, loss=3.4159\n",
      "Эпоха 1, Шаг 64451, loss=3.2742\n",
      "Эпоха 1, Шаг 64501, loss=3.5461\n",
      "Эпоха 1, Шаг 64551, loss=3.3603\n",
      "Эпоха 1, Шаг 64601, loss=3.3739\n",
      "Эпоха 1, Шаг 64651, loss=3.3602\n",
      "Эпоха 1, Шаг 64701, loss=3.3539\n",
      "Эпоха 1, Шаг 64751, loss=3.5902\n",
      "Эпоха 1, Шаг 64801, loss=3.4589\n",
      "Эпоха 1, Шаг 64851, loss=3.2784\n",
      "Эпоха 1, Шаг 64901, loss=3.3598\n",
      "Эпоха 1, Шаг 64951, loss=3.4708\n",
      "Эпоха 1, Шаг 65001, loss=3.4143\n",
      "Эпоха 1, Шаг 65051, loss=3.5389\n",
      "Эпоха 1, Шаг 65101, loss=3.4595\n",
      "Эпоха 1, Шаг 65151, loss=3.3683\n",
      "Эпоха 1, Шаг 65201, loss=3.6162\n",
      "Эпоха 1, Шаг 65251, loss=3.5567\n",
      "Эпоха 1, Шаг 65301, loss=3.4905\n",
      "Эпоха 1, Шаг 65351, loss=3.4026\n",
      "Эпоха 1, Шаг 65401, loss=3.3633\n",
      "Эпоха 1, Шаг 65451, loss=3.4002\n",
      "Эпоха 1, Шаг 65501, loss=3.4287\n",
      "Эпоха 1, Шаг 65551, loss=3.4063\n",
      "Эпоха 1, Шаг 65601, loss=3.6014\n",
      "Эпоха 1, Шаг 65651, loss=3.2883\n",
      "Эпоха 1, Шаг 65701, loss=3.4689\n",
      "Эпоха 1, Шаг 65751, loss=3.4163\n",
      "Эпоха 1, Шаг 65801, loss=3.3745\n",
      "Эпоха 1, Шаг 65851, loss=3.4025\n",
      "Эпоха 1, Шаг 65901, loss=3.3862\n",
      "Эпоха 1, Шаг 65951, loss=3.3192\n",
      "Эпоха 1, Шаг 66001, loss=3.4855\n",
      "Эпоха 1, Шаг 66051, loss=3.3162\n",
      "Эпоха 1, Шаг 66101, loss=3.4767\n",
      "Эпоха 1, Шаг 66151, loss=3.4276\n",
      "Эпоха 1, Шаг 66201, loss=3.3430\n",
      "Эпоха 1, Шаг 66251, loss=3.3508\n",
      "Эпоха 1, Шаг 66301, loss=3.3560\n",
      "Эпоха 1, Шаг 66351, loss=3.4678\n",
      "Эпоха 1, Шаг 66401, loss=3.4270\n",
      "Эпоха 1, Шаг 66451, loss=3.4830\n",
      "Эпоха 1, Шаг 66501, loss=3.3875\n",
      "Эпоха 1, Шаг 66551, loss=3.3473\n",
      "Эпоха 1, Шаг 66601, loss=3.3422\n",
      "Эпоха 1, Шаг 66651, loss=3.4829\n",
      "Эпоха 1, Шаг 66701, loss=3.3600\n",
      "Эпоха 1, Шаг 66751, loss=3.3772\n",
      "Эпоха 1, Шаг 66801, loss=3.3775\n",
      "Эпоха 1, Шаг 66851, loss=3.3373\n",
      "Эпоха 1, Шаг 66901, loss=3.3920\n",
      "Эпоха 1, Шаг 66951, loss=3.5318\n",
      "Эпоха 1, Шаг 67001, loss=3.4638\n",
      "Эпоха 1, Шаг 67051, loss=3.3310\n",
      "Эпоха 1, Шаг 67101, loss=3.3246\n",
      "Эпоха 1, Шаг 67151, loss=3.4559\n",
      "Эпоха 1, Шаг 67201, loss=3.5023\n",
      "Эпоха 1, Шаг 67251, loss=3.4115\n",
      "Эпоха 1, Шаг 67301, loss=3.3342\n",
      "Эпоха 1, Шаг 67351, loss=3.3703\n",
      "Эпоха 1, Шаг 67401, loss=3.3338\n",
      "Эпоха 1, Шаг 67451, loss=3.2729\n",
      "Эпоха 1, Шаг 67501, loss=3.4598\n",
      "Эпоха 1, Шаг 67551, loss=3.4282\n",
      "Эпоха 1, Шаг 67601, loss=3.3609\n",
      "Эпоха 1, Шаг 67651, loss=3.4565\n",
      "Эпоха 1, Шаг 67701, loss=3.3051\n",
      "Эпоха 1, Шаг 67751, loss=3.3206\n",
      "Эпоха 1, Шаг 67801, loss=3.3411\n",
      "Эпоха 1, Шаг 67851, loss=3.3592\n",
      "Эпоха 1, Шаг 67901, loss=3.4044\n",
      "Эпоха 1, Шаг 67951, loss=3.3875\n",
      "Эпоха 1, Шаг 68001, loss=3.3616\n",
      "Эпоха 1, Шаг 68051, loss=3.4536\n",
      "Эпоха 1, Шаг 68101, loss=3.4694\n",
      "Эпоха 1, Шаг 68151, loss=3.4199\n",
      "Эпоха 1, Шаг 68201, loss=3.4031\n",
      "Эпоха 1, Шаг 68251, loss=3.4116\n",
      "Эпоха 1, Шаг 68301, loss=3.4860\n",
      "Эпоха 1, Шаг 68351, loss=3.4961\n",
      "Эпоха 1, Шаг 68401, loss=3.3233\n",
      "Эпоха 1, Шаг 68451, loss=3.2627\n",
      "Эпоха 1, Шаг 68501, loss=3.2844\n",
      "Эпоха 1, Шаг 68551, loss=3.3930\n",
      "Эпоха 1, Шаг 68601, loss=3.3574\n",
      "Эпоха 1, Шаг 68651, loss=3.4280\n",
      "Эпоха 1, Шаг 68701, loss=3.3370\n",
      "Эпоха 1, Шаг 68751, loss=3.3487\n",
      "Эпоха 1, Шаг 68801, loss=3.3739\n",
      "Эпоха 1, Шаг 68851, loss=3.4184\n",
      "Эпоха 1, Шаг 68901, loss=3.4438\n",
      "Эпоха 1, Шаг 68951, loss=3.3152\n",
      "Эпоха 1, Шаг 69001, loss=3.3687\n",
      "Эпоха 1, Шаг 69051, loss=3.5560\n",
      "Эпоха 1, Шаг 69101, loss=3.3926\n",
      "Эпоха 1, Шаг 69151, loss=3.3288\n",
      "Эпоха 1, Шаг 69201, loss=3.2659\n",
      "Эпоха 1, Шаг 69251, loss=3.4680\n",
      "Эпоха 1, Шаг 69301, loss=3.2715\n",
      "Эпоха 1, Шаг 69351, loss=3.4129\n",
      "Эпоха 1, Шаг 69401, loss=3.4833\n",
      "Эпоха 1, Шаг 69451, loss=3.4268\n",
      "Эпоха 1, Шаг 69501, loss=3.3272\n",
      "Эпоха 1, Шаг 69551, loss=3.3565\n",
      "Эпоха 1, Шаг 69601, loss=3.5695\n",
      "Эпоха 1, Шаг 69651, loss=3.3311\n",
      "Эпоха 1, Шаг 69701, loss=3.4016\n",
      "Эпоха 1, Шаг 69751, loss=3.2467\n",
      "Эпоха 1, Шаг 69801, loss=3.4321\n",
      "Эпоха 1, Шаг 69851, loss=3.4081\n",
      "Эпоха 1, Шаг 69901, loss=3.3994\n",
      "Эпоха 1, Шаг 69951, loss=3.3244\n",
      "Эпоха 1, Шаг 70001, loss=3.3667\n",
      "Эпоха 1, Шаг 70051, loss=3.4975\n",
      "Эпоха 1, Шаг 70101, loss=3.2984\n",
      "Эпоха 1, Шаг 70151, loss=3.3044\n",
      "Эпоха 1, Шаг 70201, loss=3.3910\n",
      "Эпоха 1, Шаг 70251, loss=3.4012\n",
      "Эпоха 1, Шаг 70301, loss=3.4387\n",
      "Эпоха 1, Шаг 70351, loss=3.4550\n",
      "Эпоха 1, Шаг 70401, loss=3.4336\n",
      "Эпоха 1, Шаг 70451, loss=3.4414\n",
      "Эпоха 1, Шаг 70501, loss=3.4072\n",
      "Эпоха 1, Шаг 70551, loss=3.3959\n",
      "Эпоха 1, Шаг 70601, loss=3.4583\n",
      "Эпоха 1, Шаг 70651, loss=3.3551\n",
      "Эпоха 1, Шаг 70701, loss=3.3658\n",
      "Эпоха 1, Шаг 70751, loss=3.2855\n",
      "Эпоха 1, Шаг 70801, loss=3.4430\n",
      "Эпоха 1, Шаг 70851, loss=3.2954\n",
      "Эпоха 1, Шаг 70901, loss=3.3903\n",
      "Эпоха 1, Шаг 70951, loss=3.3200\n",
      "Эпоха 1, Шаг 71001, loss=3.4401\n",
      "Эпоха 1, Шаг 71051, loss=3.3664\n",
      "Эпоха 1, Шаг 71101, loss=3.3373\n",
      "Эпоха 1, Шаг 71151, loss=3.3787\n",
      "Эпоха 1, Шаг 71201, loss=3.3252\n",
      "Эпоха 1, Шаг 71251, loss=3.2423\n",
      "Эпоха 1, Шаг 71301, loss=3.3869\n",
      "Эпоха 1, Шаг 71351, loss=3.2602\n",
      "Эпоха 1, Шаг 71401, loss=3.3281\n",
      "Эпоха 1, Шаг 71451, loss=3.3686\n",
      "Эпоха 1, Шаг 71501, loss=3.3261\n",
      "Эпоха 1, Шаг 71551, loss=3.4342\n",
      "Эпоха 1, Шаг 71601, loss=3.3417\n",
      "Эпоха 1, Шаг 71651, loss=3.3014\n",
      "Эпоха 1, Шаг 71701, loss=3.2095\n",
      "Эпоха 1, Шаг 71751, loss=3.3692\n",
      "Эпоха 1, Шаг 71801, loss=3.4263\n",
      "Эпоха 1, Шаг 71851, loss=3.4045\n",
      "Эпоха 1, Шаг 71901, loss=3.3140\n",
      "Эпоха 1, Шаг 71951, loss=3.3174\n",
      "Эпоха 1, Шаг 72001, loss=3.5015\n",
      "Эпоха 1, Шаг 72051, loss=3.3144\n",
      "Эпоха 1, Шаг 72101, loss=3.5117\n",
      "Эпоха 1, Шаг 72151, loss=3.4509\n",
      "Эпоха 1, Шаг 72201, loss=3.3206\n",
      "Эпоха 1, Шаг 72251, loss=3.3609\n",
      "Эпоха 1, Шаг 72301, loss=3.3856\n",
      "Эпоха 1, Шаг 72351, loss=3.3995\n",
      "Эпоха 1, Шаг 72401, loss=3.4235\n",
      "Эпоха 1, Шаг 72451, loss=3.4686\n",
      "Эпоха 1, Шаг 72501, loss=3.4115\n",
      "Эпоха 1, Шаг 72551, loss=3.3526\n",
      "Эпоха 1, Шаг 72601, loss=3.3007\n",
      "Эпоха 1, Шаг 72651, loss=3.4022\n",
      "Эпоха 1, Шаг 72701, loss=3.4018\n",
      "Эпоха 1, Шаг 72751, loss=3.4147\n",
      "Эпоха 1, Шаг 72801, loss=3.3333\n",
      "Эпоха 1, Шаг 72851, loss=3.2706\n",
      "Эпоха 1, Шаг 72901, loss=3.2511\n",
      "Эпоха 1, Шаг 72951, loss=3.3310\n",
      "Эпоха 1, Шаг 73001, loss=3.3190\n",
      "Эпоха 1, Шаг 73051, loss=3.1701\n",
      "Эпоха 1, Шаг 73101, loss=3.3158\n",
      "Эпоха 1, Шаг 73151, loss=3.4111\n",
      "Эпоха 1, Шаг 73201, loss=3.3937\n",
      "Эпоха 1, Шаг 73251, loss=3.5558\n",
      "Эпоха 1, Шаг 73301, loss=3.4762\n",
      "Эпоха 1, Шаг 73351, loss=3.3796\n",
      "Эпоха 1, Шаг 73401, loss=3.3436\n",
      "Эпоха 1, Шаг 73451, loss=3.2758\n",
      "Эпоха 1, Шаг 73501, loss=3.3841\n",
      "Эпоха 1, Шаг 73551, loss=3.4979\n",
      "Эпоха 1, Шаг 73601, loss=3.3251\n",
      "Эпоха 1, Шаг 73651, loss=3.2798\n",
      "Эпоха 1, Шаг 73701, loss=3.4001\n",
      "Эпоха 1, Шаг 73751, loss=3.3671\n",
      "Эпоха 1, Шаг 73801, loss=3.4918\n",
      "Эпоха 1, Шаг 73851, loss=3.3768\n",
      "Эпоха 1, Шаг 73901, loss=3.3896\n",
      "Эпоха 1, Шаг 73951, loss=3.3761\n",
      "Эпоха 1, Шаг 74001, loss=3.2549\n",
      "Эпоха 1, Шаг 74051, loss=3.2868\n",
      "Эпоха 1, Шаг 74101, loss=3.3800\n",
      "Эпоха 1, Шаг 74151, loss=3.3252\n",
      "Эпоха 1, Шаг 74201, loss=3.3398\n",
      "Эпоха 1, Шаг 74251, loss=3.1964\n",
      "Эпоха 1, Шаг 74301, loss=3.3494\n",
      "Эпоха 1, Шаг 74351, loss=3.2756\n",
      "Эпоха 1, Шаг 74401, loss=3.3119\n",
      "Эпоха 1, Шаг 74451, loss=3.3928\n",
      "Эпоха 1, Шаг 74501, loss=3.4493\n",
      "Эпоха 1, Шаг 74551, loss=3.4197\n",
      "Эпоха 1, Шаг 74601, loss=3.3934\n",
      "Эпоха 1, Шаг 74651, loss=3.3717\n",
      "Эпоха 1, Шаг 74701, loss=3.4452\n",
      "Эпоха 1, Шаг 74751, loss=3.2770\n",
      "Эпоха 1, Шаг 74801, loss=3.4145\n",
      "Эпоха 1, Шаг 74851, loss=3.3919\n",
      "Эпоха 1, Шаг 74901, loss=3.4782\n",
      "Эпоха 1, Шаг 74951, loss=3.3544\n",
      "Эпоха 1, Шаг 75001, loss=3.4439\n",
      "Эпоха 1, Шаг 75051, loss=3.4018\n",
      "Эпоха 1, Шаг 75101, loss=3.4570\n",
      "Эпоха 1, Шаг 75151, loss=3.3284\n",
      "Эпоха 1, Шаг 75201, loss=3.3326\n",
      "Эпоха 1, Шаг 75251, loss=3.3162\n",
      "Эпоха 1, Шаг 75301, loss=3.3812\n",
      "Эпоха 1, Шаг 75351, loss=3.3670\n",
      "Эпоха 1, Шаг 75401, loss=3.3576\n",
      "Эпоха 1, Шаг 75451, loss=3.2056\n",
      "Эпоха 1, Шаг 75501, loss=3.1653\n",
      "Эпоха 1, Шаг 75551, loss=3.4383\n",
      "Эпоха 1, Шаг 75601, loss=3.4100\n",
      "Эпоха 1, Шаг 75651, loss=3.3616\n",
      "Эпоха 1, Шаг 75701, loss=3.2672\n",
      "Эпоха 1, Шаг 75751, loss=3.3302\n",
      "Эпоха 1, Шаг 75801, loss=3.4777\n",
      "Эпоха 1, Шаг 75851, loss=3.4775\n",
      "Эпоха 1, Шаг 75901, loss=3.3975\n",
      "Эпоха 1, Шаг 75951, loss=3.2930\n",
      "Эпоха 1, Шаг 76001, loss=3.3189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Эпоха 1, Шаг 76051, loss=3.1933\n",
      "Эпоха 1, Шаг 76101, loss=3.4058\n",
      "Эпоха 1, Шаг 76151, loss=3.4049\n",
      "Эпоха 1, Шаг 76201, loss=3.2502\n",
      "Эпоха 1, Шаг 76251, loss=3.2857\n",
      "Эпоха 1, Шаг 76301, loss=3.3906\n",
      "Эпоха 1, Шаг 76351, loss=3.3439\n",
      "Эпоха 1, Шаг 76401, loss=3.3518\n",
      "Эпоха 1, Шаг 76451, loss=3.3147\n",
      "Эпоха 1, Шаг 76501, loss=3.4507\n",
      "Эпоха 1, Шаг 76551, loss=3.3839\n",
      "Эпоха 1, Шаг 76601, loss=3.3286\n",
      "Эпоха 1, Шаг 76651, loss=3.4032\n",
      "Эпоха 1, Шаг 76701, loss=3.4540\n",
      "Эпоха 1, Шаг 76751, loss=3.2981\n",
      "Эпоха 1, Шаг 76801, loss=3.4177\n",
      "Эпоха 1, Шаг 76851, loss=3.2709\n",
      "Эпоха 1, Шаг 76901, loss=3.2632\n",
      "Эпоха 1, Шаг 76951, loss=3.3812\n",
      "Эпоха 1, Шаг 77001, loss=3.3186\n",
      "Эпоха 1, Шаг 77051, loss=3.3483\n",
      "Эпоха 1, Шаг 77101, loss=3.4250\n",
      "Эпоха 1, Шаг 77151, loss=3.2730\n",
      "Эпоха 1, Шаг 77201, loss=3.3451\n",
      "Эпоха 1, Шаг 77251, loss=3.4072\n",
      "Эпоха 1, Шаг 77301, loss=3.1913\n",
      "Эпоха 1, Шаг 77351, loss=3.3771\n",
      "Эпоха 1, Шаг 77401, loss=3.4600\n",
      "Эпоха 1, Шаг 77451, loss=3.2499\n",
      "Эпоха 1, Шаг 77501, loss=3.4119\n",
      "Эпоха 1, Шаг 77551, loss=3.2798\n",
      "Эпоха 1, Шаг 77601, loss=3.2194\n",
      "Эпоха 1, Шаг 77651, loss=3.2554\n",
      "Эпоха 1, Шаг 77701, loss=3.3416\n",
      "Эпоха 1, Шаг 77751, loss=3.3673\n",
      "Эпоха 1, Шаг 77801, loss=3.3137\n",
      "Эпоха 1, Шаг 77851, loss=3.3168\n",
      "Эпоха 1, Шаг 77901, loss=3.4276\n",
      "Эпоха 1, Шаг 77951, loss=3.3299\n",
      "Эпоха 1, Шаг 78001, loss=3.3605\n",
      "Эпоха 1, Шаг 78051, loss=3.3555\n",
      "Эпоха 1, Шаг 78101, loss=3.2212\n",
      "Эпоха 1, Шаг 78151, loss=3.3821\n",
      "Эпоха 1, Шаг 78201, loss=3.2993\n",
      "Эпоха 1, Шаг 78251, loss=3.3552\n",
      "Эпоха 1, Шаг 78301, loss=3.4358\n",
      "Эпоха 1, Шаг 78351, loss=3.3322\n",
      "Эпоха 1, Шаг 78401, loss=3.3317\n",
      "Эпоха 1, Шаг 78451, loss=3.4154\n",
      "Эпоха 1, Шаг 78501, loss=3.2604\n",
      "Эпоха 1, Шаг 78551, loss=3.3005\n",
      "Эпоха 1, Шаг 78601, loss=3.3039\n",
      "Эпоха 1, Шаг 78651, loss=3.4330\n",
      "Эпоха 1, Шаг 78701, loss=3.3903\n",
      "Эпоха 1, Шаг 78751, loss=3.3642\n",
      "Эпоха 1, Шаг 78801, loss=3.4492\n",
      "Эпоха 1, Шаг 78851, loss=3.3696\n",
      "Эпоха 1, Шаг 78901, loss=3.3340\n",
      "Эпоха 1, Шаг 78951, loss=3.3499\n",
      "Эпоха 1, Шаг 79001, loss=3.3590\n",
      "Эпоха 1, Шаг 79051, loss=3.3539\n",
      "Эпоха 1, Шаг 79101, loss=3.3449\n",
      "Эпоха 1, Шаг 79151, loss=3.3838\n",
      "Эпоха 1, Шаг 79201, loss=3.3737\n",
      "Эпоха 1, Шаг 79251, loss=3.5239\n",
      "Эпоха 1, Шаг 79301, loss=3.3484\n",
      "Эпоха 1, Шаг 79351, loss=3.3827\n",
      "Эпоха 1, Шаг 79401, loss=3.4427\n",
      "Эпоха 1, Шаг 79451, loss=3.3968\n",
      "Эпоха 1, Шаг 79501, loss=3.4895\n",
      "Эпоха 1, Шаг 79551, loss=3.4009\n",
      "Эпоха 1, Шаг 79601, loss=3.4499\n",
      "Эпоха 1, Шаг 79651, loss=3.3597\n",
      "Эпоха 1, Шаг 79701, loss=3.3063\n",
      "Эпоха 1, Шаг 79751, loss=3.3357\n",
      "Эпоха 1, Шаг 79801, loss=3.3770\n",
      "Эпоха 1, Шаг 79851, loss=3.3155\n",
      "Эпоха 1, Шаг 79901, loss=3.3739\n",
      "Эпоха 1, Шаг 79951, loss=3.3469\n",
      "Эпоха 1, Шаг 80001, loss=3.3624\n",
      "Эпоха 1, Шаг 80051, loss=3.4381\n",
      "Эпоха 1, Шаг 80101, loss=3.3600\n",
      "Эпоха 1, Шаг 80151, loss=3.2357\n",
      "Эпоха 1, Шаг 80201, loss=3.3612\n",
      "Эпоха 1, Шаг 80251, loss=3.2587\n",
      "Эпоха 1, Шаг 80301, loss=3.3698\n",
      "Эпоха 1, Шаг 80351, loss=3.3522\n",
      "Эпоха 1, Шаг 80401, loss=3.2883\n",
      "Эпоха 1, Шаг 80451, loss=3.3475\n",
      "Эпоха 1, Шаг 80501, loss=3.4036\n",
      "Эпоха 1, Шаг 80551, loss=3.2718\n",
      "Эпоха 1, Шаг 80601, loss=3.2784\n",
      "Эпоха 1, Шаг 80651, loss=3.5220\n",
      "Эпоха 1, Шаг 80701, loss=3.2813\n",
      "Эпоха 1, Шаг 80751, loss=3.3793\n",
      "Эпоха 1, Шаг 80801, loss=3.2686\n",
      "Эпоха 1, Шаг 80851, loss=3.3894\n",
      "Эпоха 1, Шаг 80901, loss=3.2512\n",
      "Эпоха 1, Шаг 80951, loss=3.4161\n",
      "Эпоха 1, Шаг 81001, loss=3.3069\n",
      "Эпоха 1, Шаг 81051, loss=3.3265\n",
      "Эпоха 1, Шаг 81101, loss=3.2970\n",
      "Эпоха 1 завершена, avg loss=3.7148\n",
      "Эпоха 2, Шаг 81145, loss=3.4069\n",
      "Эпоха 2, Шаг 81195, loss=3.3081\n",
      "Эпоха 2, Шаг 81245, loss=3.4265\n",
      "Эпоха 2, Шаг 81295, loss=3.2645\n",
      "Эпоха 2, Шаг 81345, loss=3.3371\n",
      "Эпоха 2, Шаг 81395, loss=3.4641\n",
      "Эпоха 2, Шаг 81445, loss=3.5942\n",
      "Эпоха 2, Шаг 81495, loss=3.4042\n",
      "Эпоха 2, Шаг 81545, loss=3.2949\n",
      "Эпоха 2, Шаг 81595, loss=3.4070\n",
      "Эпоха 2, Шаг 81645, loss=3.2792\n",
      "Эпоха 2, Шаг 81695, loss=3.3563\n",
      "Эпоха 2, Шаг 81745, loss=3.2473\n",
      "Эпоха 2, Шаг 81795, loss=3.4514\n",
      "Эпоха 2, Шаг 81845, loss=3.3309\n",
      "Эпоха 2, Шаг 81895, loss=3.3638\n",
      "Эпоха 2, Шаг 81945, loss=3.5427\n",
      "Эпоха 2, Шаг 81995, loss=3.2720\n",
      "Эпоха 2, Шаг 82045, loss=3.3943\n",
      "Эпоха 2, Шаг 82095, loss=3.2137\n",
      "Эпоха 2, Шаг 82145, loss=3.2066\n",
      "Эпоха 2, Шаг 82195, loss=3.3058\n",
      "Эпоха 2, Шаг 82245, loss=3.3083\n",
      "Эпоха 2, Шаг 82295, loss=3.4828\n",
      "Эпоха 2, Шаг 82345, loss=3.1768\n",
      "Эпоха 2, Шаг 82395, loss=3.3759\n",
      "Эпоха 2, Шаг 82445, loss=3.3779\n",
      "Эпоха 2, Шаг 82495, loss=3.2845\n",
      "Эпоха 2, Шаг 82545, loss=3.4176\n",
      "Эпоха 2, Шаг 82595, loss=3.4237\n",
      "Эпоха 2, Шаг 82645, loss=3.1014\n",
      "Эпоха 2, Шаг 82695, loss=3.1974\n",
      "Эпоха 2, Шаг 82745, loss=3.3413\n",
      "Эпоха 2, Шаг 82795, loss=3.3578\n",
      "Эпоха 2, Шаг 82845, loss=3.3188\n",
      "Эпоха 2, Шаг 82895, loss=3.3378\n",
      "Эпоха 2, Шаг 82945, loss=3.2503\n",
      "Эпоха 2, Шаг 82995, loss=3.4009\n",
      "Эпоха 2, Шаг 83045, loss=3.2059\n",
      "Эпоха 2, Шаг 83095, loss=3.2944\n",
      "Эпоха 2, Шаг 83145, loss=3.3183\n",
      "Эпоха 2, Шаг 83195, loss=3.1891\n",
      "Эпоха 2, Шаг 83245, loss=3.2245\n",
      "Эпоха 2, Шаг 83295, loss=3.3749\n",
      "Эпоха 2, Шаг 83345, loss=3.1993\n",
      "Эпоха 2, Шаг 83395, loss=3.2633\n",
      "Эпоха 2, Шаг 83445, loss=3.4069\n",
      "Эпоха 2, Шаг 83495, loss=3.3382\n",
      "Эпоха 2, Шаг 83545, loss=3.2702\n",
      "Эпоха 2, Шаг 83595, loss=3.3465\n",
      "Эпоха 2, Шаг 83645, loss=3.3463\n",
      "Эпоха 2, Шаг 83695, loss=3.3385\n",
      "Эпоха 2, Шаг 83745, loss=3.4027\n",
      "Эпоха 2, Шаг 83795, loss=3.2874\n",
      "Эпоха 2, Шаг 83845, loss=3.3136\n",
      "Эпоха 2, Шаг 83895, loss=3.2743\n",
      "Эпоха 2, Шаг 83945, loss=3.3557\n",
      "Эпоха 2, Шаг 83995, loss=3.3139\n",
      "Эпоха 2, Шаг 84045, loss=3.3891\n",
      "Эпоха 2, Шаг 84095, loss=3.3025\n",
      "Эпоха 2, Шаг 84145, loss=3.2990\n",
      "Эпоха 2, Шаг 84195, loss=3.3057\n",
      "Эпоха 2, Шаг 84245, loss=3.3686\n",
      "Эпоха 2, Шаг 84295, loss=3.3286\n",
      "Эпоха 2, Шаг 84345, loss=3.4100\n",
      "Эпоха 2, Шаг 84395, loss=3.3760\n",
      "Эпоха 2, Шаг 84445, loss=3.3902\n",
      "Эпоха 2, Шаг 84495, loss=3.2975\n",
      "Эпоха 2, Шаг 84545, loss=3.2885\n",
      "Эпоха 2, Шаг 84595, loss=3.3884\n",
      "Эпоха 2, Шаг 84645, loss=3.2406\n",
      "Эпоха 2, Шаг 84695, loss=3.4106\n",
      "Эпоха 2, Шаг 84745, loss=3.4250\n",
      "Эпоха 2, Шаг 84795, loss=3.3172\n",
      "Эпоха 2, Шаг 84845, loss=3.3685\n",
      "Эпоха 2, Шаг 84895, loss=3.3262\n",
      "Эпоха 2, Шаг 84945, loss=3.3481\n",
      "Эпоха 2, Шаг 84995, loss=3.3305\n",
      "Эпоха 2, Шаг 85045, loss=3.2803\n",
      "Эпоха 2, Шаг 85095, loss=3.3716\n",
      "Эпоха 2, Шаг 85145, loss=3.3142\n",
      "Эпоха 2, Шаг 85195, loss=3.2894\n",
      "Эпоха 2, Шаг 85245, loss=3.2076\n",
      "Эпоха 2, Шаг 85295, loss=3.2636\n",
      "Эпоха 2, Шаг 85345, loss=3.2234\n",
      "Эпоха 2, Шаг 85395, loss=3.2535\n",
      "Эпоха 2, Шаг 85445, loss=3.2572\n",
      "Эпоха 2, Шаг 85495, loss=3.2461\n",
      "Эпоха 2, Шаг 85545, loss=3.3117\n",
      "Эпоха 2, Шаг 85595, loss=3.3096\n",
      "Эпоха 2, Шаг 85645, loss=3.2166\n",
      "Эпоха 2, Шаг 85695, loss=3.4074\n",
      "Эпоха 2, Шаг 85745, loss=3.2879\n",
      "Эпоха 2, Шаг 85795, loss=3.2925\n",
      "Эпоха 2, Шаг 85845, loss=3.3554\n",
      "Эпоха 2, Шаг 85895, loss=3.4595\n",
      "Эпоха 2, Шаг 85945, loss=3.3362\n",
      "Эпоха 2, Шаг 85995, loss=3.4369\n",
      "Эпоха 2, Шаг 86045, loss=3.3374\n",
      "Эпоха 2, Шаг 86095, loss=3.2746\n",
      "Эпоха 2, Шаг 86145, loss=3.2412\n",
      "Эпоха 2, Шаг 86195, loss=3.1686\n",
      "Эпоха 2, Шаг 86245, loss=3.1960\n",
      "Эпоха 2, Шаг 86295, loss=3.3482\n",
      "Эпоха 2, Шаг 86345, loss=3.4264\n",
      "Эпоха 2, Шаг 86395, loss=3.2192\n",
      "Эпоха 2, Шаг 86445, loss=3.3391\n",
      "Эпоха 2, Шаг 86495, loss=3.3184\n",
      "Эпоха 2, Шаг 86545, loss=3.3062\n",
      "Эпоха 2, Шаг 86595, loss=3.3067\n",
      "Эпоха 2, Шаг 86645, loss=3.3599\n",
      "Эпоха 2, Шаг 86695, loss=3.3627\n",
      "Эпоха 2, Шаг 86745, loss=3.4007\n",
      "Эпоха 2, Шаг 86795, loss=3.1717\n",
      "Эпоха 2, Шаг 86845, loss=3.3962\n",
      "Эпоха 2, Шаг 86895, loss=3.4105\n",
      "Эпоха 2, Шаг 86945, loss=3.3377\n",
      "Эпоха 2, Шаг 86995, loss=3.3055\n",
      "Эпоха 2, Шаг 87045, loss=3.2838\n",
      "Эпоха 2, Шаг 87095, loss=3.3706\n",
      "Эпоха 2, Шаг 87145, loss=3.2205\n",
      "Эпоха 2, Шаг 87195, loss=3.2285\n",
      "Эпоха 2, Шаг 87245, loss=3.3624\n",
      "Эпоха 2, Шаг 87295, loss=3.3489\n",
      "Эпоха 2, Шаг 87345, loss=3.2890\n",
      "Эпоха 2, Шаг 87395, loss=3.3972\n",
      "Эпоха 2, Шаг 87445, loss=3.3025\n",
      "Эпоха 2, Шаг 87495, loss=3.2309\n",
      "Эпоха 2, Шаг 87545, loss=3.3977\n",
      "Эпоха 2, Шаг 87595, loss=3.3550\n",
      "Эпоха 2, Шаг 87645, loss=3.2784\n",
      "Эпоха 2, Шаг 87695, loss=3.2687\n",
      "Эпоха 2, Шаг 87745, loss=3.3229\n",
      "Эпоха 2, Шаг 87795, loss=3.3053\n",
      "Эпоха 2, Шаг 87845, loss=3.4732\n",
      "Эпоха 2, Шаг 87895, loss=3.4742\n",
      "Эпоха 2, Шаг 87945, loss=3.3349\n",
      "Эпоха 2, Шаг 87995, loss=3.2475\n",
      "Эпоха 2, Шаг 88045, loss=3.2462\n",
      "Эпоха 2, Шаг 88095, loss=3.1307\n",
      "Эпоха 2, Шаг 88145, loss=3.3494\n",
      "Эпоха 2, Шаг 88195, loss=3.3967\n",
      "Эпоха 2, Шаг 88245, loss=3.2492\n",
      "Эпоха 2, Шаг 88295, loss=3.4577\n",
      "Эпоха 2, Шаг 88345, loss=3.3550\n",
      "Эпоха 2, Шаг 88395, loss=3.2599\n",
      "Эпоха 2, Шаг 88445, loss=3.4303\n",
      "Эпоха 2, Шаг 88495, loss=3.2136\n",
      "Эпоха 2, Шаг 88545, loss=3.3291\n",
      "Эпоха 2, Шаг 88595, loss=3.2132\n",
      "Эпоха 2, Шаг 88645, loss=3.2960\n",
      "Эпоха 2, Шаг 88695, loss=3.1927\n",
      "Эпоха 2, Шаг 88745, loss=3.2976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Эпоха 2, Шаг 88795, loss=3.3778\n",
      "Эпоха 2, Шаг 88845, loss=3.3209\n",
      "Эпоха 2, Шаг 88895, loss=3.3234\n",
      "Эпоха 2, Шаг 88945, loss=3.2008\n",
      "Эпоха 2, Шаг 88995, loss=3.3115\n",
      "Эпоха 2, Шаг 89045, loss=3.1496\n",
      "Эпоха 2, Шаг 89095, loss=3.2908\n",
      "Эпоха 2, Шаг 89145, loss=3.3122\n",
      "Эпоха 2, Шаг 89195, loss=3.2753\n",
      "Эпоха 2, Шаг 89245, loss=3.4002\n",
      "Эпоха 2, Шаг 89295, loss=3.4123\n",
      "Эпоха 2, Шаг 89345, loss=3.3842\n",
      "Эпоха 2, Шаг 89395, loss=3.3736\n",
      "Эпоха 2, Шаг 89445, loss=3.3019\n",
      "Эпоха 2, Шаг 89495, loss=3.3733\n",
      "Эпоха 2, Шаг 89545, loss=3.0918\n",
      "Эпоха 2, Шаг 89595, loss=3.1713\n",
      "Эпоха 2, Шаг 89645, loss=3.2708\n",
      "Эпоха 2, Шаг 89695, loss=3.3713\n",
      "Эпоха 2, Шаг 89745, loss=3.3210\n",
      "Эпоха 2, Шаг 89795, loss=3.4331\n",
      "Эпоха 2, Шаг 89845, loss=3.2477\n",
      "Эпоха 2, Шаг 89895, loss=3.3363\n",
      "Эпоха 2, Шаг 89945, loss=3.4216\n",
      "Эпоха 2, Шаг 89995, loss=3.4274\n",
      "Эпоха 2, Шаг 90045, loss=3.2968\n",
      "Эпоха 2, Шаг 90095, loss=3.4573\n",
      "Эпоха 2, Шаг 90145, loss=3.3074\n",
      "Эпоха 2, Шаг 90195, loss=3.3288\n",
      "Эпоха 2, Шаг 90245, loss=3.3249\n",
      "Эпоха 2, Шаг 90295, loss=3.1910\n",
      "Эпоха 2, Шаг 90345, loss=3.3029\n",
      "Эпоха 2, Шаг 90395, loss=3.2413\n",
      "Эпоха 2, Шаг 90445, loss=3.2583\n",
      "Эпоха 2, Шаг 90495, loss=3.3470\n",
      "Эпоха 2, Шаг 90545, loss=3.3541\n",
      "Эпоха 2, Шаг 90595, loss=3.2969\n",
      "Эпоха 2, Шаг 90645, loss=3.3830\n",
      "Эпоха 2, Шаг 90695, loss=3.3064\n",
      "Эпоха 2, Шаг 90745, loss=3.2796\n",
      "Эпоха 2, Шаг 90795, loss=3.4308\n",
      "Эпоха 2, Шаг 90845, loss=3.3296\n",
      "Эпоха 2, Шаг 90895, loss=3.3389\n",
      "Эпоха 2, Шаг 90945, loss=3.2101\n",
      "Эпоха 2, Шаг 90995, loss=3.3034\n",
      "Эпоха 2, Шаг 91045, loss=3.2366\n",
      "Эпоха 2, Шаг 91095, loss=3.1689\n",
      "Эпоха 2, Шаг 91145, loss=3.3294\n",
      "Эпоха 2, Шаг 91195, loss=3.3719\n",
      "Эпоха 2, Шаг 91245, loss=3.4209\n",
      "Эпоха 2, Шаг 91295, loss=3.3293\n",
      "Эпоха 2, Шаг 91345, loss=3.3066\n",
      "Эпоха 2, Шаг 91395, loss=3.1957\n",
      "Эпоха 2, Шаг 91445, loss=3.2859\n",
      "Эпоха 2, Шаг 91495, loss=3.4052\n",
      "Эпоха 2, Шаг 91545, loss=3.3991\n",
      "Эпоха 2, Шаг 91595, loss=3.3344\n",
      "Эпоха 2, Шаг 91645, loss=3.3025\n",
      "Эпоха 2, Шаг 91695, loss=3.3029\n",
      "Эпоха 2, Шаг 91745, loss=3.1898\n",
      "Эпоха 2, Шаг 91795, loss=3.1933\n",
      "Эпоха 2, Шаг 91845, loss=3.3072\n",
      "Эпоха 2, Шаг 91895, loss=3.2138\n",
      "Эпоха 2, Шаг 91945, loss=3.4059\n",
      "Эпоха 2, Шаг 91995, loss=3.1631\n",
      "Эпоха 2, Шаг 92045, loss=3.4223\n",
      "Эпоха 2, Шаг 92095, loss=3.1724\n",
      "Эпоха 2, Шаг 92145, loss=3.2164\n",
      "Эпоха 2, Шаг 92195, loss=3.2261\n",
      "Эпоха 2, Шаг 92245, loss=3.2399\n",
      "Эпоха 2, Шаг 92295, loss=3.2817\n",
      "Эпоха 2, Шаг 92345, loss=3.3945\n",
      "Эпоха 2, Шаг 92395, loss=3.4302\n",
      "Эпоха 2, Шаг 92445, loss=3.3137\n",
      "Эпоха 2, Шаг 92495, loss=3.2157\n",
      "Эпоха 2, Шаг 92545, loss=3.2617\n",
      "Эпоха 2, Шаг 92595, loss=3.3863\n",
      "Эпоха 2, Шаг 92645, loss=3.2607\n",
      "Эпоха 2, Шаг 92695, loss=3.2592\n",
      "Эпоха 2, Шаг 92745, loss=3.3128\n",
      "Эпоха 2, Шаг 92795, loss=3.3664\n",
      "Эпоха 2, Шаг 92845, loss=3.3458\n",
      "Эпоха 2, Шаг 92895, loss=3.3901\n",
      "Эпоха 2, Шаг 92945, loss=3.1987\n",
      "Эпоха 2, Шаг 92995, loss=3.3383\n",
      "Эпоха 2, Шаг 93045, loss=3.3267\n",
      "Эпоха 2, Шаг 93095, loss=3.3924\n",
      "Эпоха 2, Шаг 93145, loss=3.2161\n",
      "Эпоха 2, Шаг 93195, loss=3.2487\n",
      "Эпоха 2, Шаг 93245, loss=3.2609\n",
      "Эпоха 2, Шаг 93295, loss=3.2595\n",
      "Эпоха 2, Шаг 93345, loss=3.2639\n",
      "Эпоха 2, Шаг 93395, loss=3.2930\n",
      "Эпоха 2, Шаг 93445, loss=3.4530\n",
      "Эпоха 2, Шаг 93495, loss=3.1666\n",
      "Эпоха 2, Шаг 93545, loss=3.4110\n",
      "Эпоха 2, Шаг 93595, loss=3.2005\n",
      "Эпоха 2, Шаг 93645, loss=3.2836\n",
      "Эпоха 2, Шаг 93695, loss=3.3296\n",
      "Эпоха 2, Шаг 93745, loss=3.4008\n",
      "Эпоха 2, Шаг 93795, loss=3.3157\n",
      "Эпоха 2, Шаг 93845, loss=3.2949\n",
      "Эпоха 2, Шаг 93895, loss=3.2903\n",
      "Эпоха 2, Шаг 93945, loss=3.2402\n",
      "Эпоха 2, Шаг 93995, loss=3.3101\n",
      "Эпоха 2, Шаг 94045, loss=3.3508\n",
      "Эпоха 2, Шаг 94095, loss=3.3580\n",
      "Эпоха 2, Шаг 94145, loss=3.2799\n",
      "Эпоха 2, Шаг 94195, loss=3.4111\n",
      "Эпоха 2, Шаг 94245, loss=3.2587\n",
      "Эпоха 2, Шаг 94295, loss=3.2435\n",
      "Эпоха 2, Шаг 94345, loss=3.3696\n",
      "Эпоха 2, Шаг 94395, loss=3.3527\n",
      "Эпоха 2, Шаг 94445, loss=3.3655\n",
      "Эпоха 2, Шаг 94495, loss=3.2220\n",
      "Эпоха 2, Шаг 94545, loss=3.2886\n",
      "Эпоха 2, Шаг 94595, loss=3.2481\n",
      "Эпоха 2, Шаг 94645, loss=3.3358\n",
      "Эпоха 2, Шаг 94695, loss=3.3477\n",
      "Эпоха 2, Шаг 94745, loss=3.3030\n",
      "Эпоха 2, Шаг 94795, loss=3.4012\n",
      "Эпоха 2, Шаг 94845, loss=3.5001\n",
      "Эпоха 2, Шаг 94895, loss=3.2507\n",
      "Эпоха 2, Шаг 94945, loss=3.4146\n",
      "Эпоха 2, Шаг 94995, loss=3.3340\n",
      "Эпоха 2, Шаг 95045, loss=3.2249\n",
      "Эпоха 2, Шаг 95095, loss=3.3269\n",
      "Эпоха 2, Шаг 95145, loss=3.3217\n",
      "Эпоха 2, Шаг 95195, loss=3.3943\n",
      "Эпоха 2, Шаг 95245, loss=3.1786\n",
      "Эпоха 2, Шаг 95295, loss=3.3199\n",
      "Эпоха 2, Шаг 95345, loss=3.2526\n",
      "Эпоха 2, Шаг 95395, loss=3.3648\n",
      "Эпоха 2, Шаг 95445, loss=3.3798\n",
      "Эпоха 2, Шаг 95495, loss=3.2849\n",
      "Эпоха 2, Шаг 95545, loss=3.3439\n",
      "Эпоха 2, Шаг 95595, loss=3.3450\n",
      "Эпоха 2, Шаг 95645, loss=3.2326\n",
      "Эпоха 2, Шаг 95695, loss=3.2779\n",
      "Эпоха 2, Шаг 95745, loss=3.4110\n",
      "Эпоха 2, Шаг 95795, loss=3.2552\n",
      "Эпоха 2, Шаг 95845, loss=3.1658\n",
      "Эпоха 2, Шаг 95895, loss=3.3079\n",
      "Эпоха 2, Шаг 95945, loss=3.3268\n",
      "Эпоха 2, Шаг 95995, loss=3.2660\n",
      "Эпоха 2, Шаг 96045, loss=3.2268\n",
      "Эпоха 2, Шаг 96095, loss=3.3203\n",
      "Эпоха 2, Шаг 96145, loss=3.3255\n",
      "Эпоха 2, Шаг 96195, loss=3.3368\n",
      "Эпоха 2, Шаг 96245, loss=3.3079\n",
      "Эпоха 2, Шаг 96295, loss=3.2534\n",
      "Эпоха 2, Шаг 96345, loss=3.3222\n",
      "Эпоха 2, Шаг 96395, loss=3.1313\n",
      "Эпоха 2, Шаг 96445, loss=3.2658\n",
      "Эпоха 2, Шаг 96495, loss=3.2788\n",
      "Эпоха 2, Шаг 96545, loss=3.2764\n",
      "Эпоха 2, Шаг 96595, loss=3.2827\n",
      "Эпоха 2, Шаг 96645, loss=3.3235\n",
      "Эпоха 2, Шаг 96695, loss=3.2689\n",
      "Эпоха 2, Шаг 96745, loss=3.3553\n",
      "Эпоха 2, Шаг 96795, loss=3.2398\n",
      "Эпоха 2, Шаг 96845, loss=3.3004\n",
      "Эпоха 2, Шаг 96895, loss=3.3245\n",
      "Эпоха 2, Шаг 96945, loss=3.3615\n",
      "Эпоха 2, Шаг 96995, loss=3.3920\n",
      "Эпоха 2, Шаг 97045, loss=3.3713\n",
      "Эпоха 2, Шаг 97095, loss=3.3339\n",
      "Эпоха 2, Шаг 97145, loss=3.2046\n",
      "Эпоха 2, Шаг 97195, loss=3.3600\n",
      "Эпоха 2, Шаг 97245, loss=3.3458\n",
      "Эпоха 2, Шаг 97295, loss=3.2837\n",
      "Эпоха 2, Шаг 97345, loss=3.3165\n",
      "Эпоха 2, Шаг 97395, loss=3.3378\n",
      "Эпоха 2, Шаг 97445, loss=3.2679\n",
      "Эпоха 2, Шаг 97495, loss=3.2165\n",
      "Эпоха 2, Шаг 97545, loss=3.3677\n",
      "Эпоха 2, Шаг 97595, loss=3.2687\n",
      "Эпоха 2, Шаг 97645, loss=3.3131\n",
      "Эпоха 2, Шаг 97695, loss=3.2200\n",
      "Эпоха 2, Шаг 97745, loss=3.2287\n",
      "Эпоха 2, Шаг 97795, loss=3.2022\n",
      "Эпоха 2, Шаг 97845, loss=3.2239\n",
      "Эпоха 2, Шаг 97895, loss=3.2691\n",
      "Эпоха 2, Шаг 97945, loss=3.3485\n",
      "Эпоха 2, Шаг 97995, loss=3.1796\n",
      "Эпоха 2, Шаг 98045, loss=3.3172\n",
      "Эпоха 2, Шаг 98095, loss=3.3933\n",
      "Эпоха 2, Шаг 98145, loss=3.2950\n",
      "Эпоха 2, Шаг 98195, loss=3.3091\n",
      "Эпоха 2, Шаг 98245, loss=3.3494\n",
      "Эпоха 2, Шаг 98295, loss=3.2748\n",
      "Эпоха 2, Шаг 98345, loss=3.1444\n",
      "Эпоха 2, Шаг 98395, loss=3.3882\n",
      "Эпоха 2, Шаг 98445, loss=3.2930\n",
      "Эпоха 2, Шаг 98495, loss=3.2281\n",
      "Эпоха 2, Шаг 98545, loss=3.3852\n",
      "Эпоха 2, Шаг 98595, loss=3.2581\n",
      "Эпоха 2, Шаг 98645, loss=3.3033\n",
      "Эпоха 2, Шаг 98695, loss=3.1806\n",
      "Эпоха 2, Шаг 98745, loss=3.1235\n",
      "Эпоха 2, Шаг 98795, loss=3.2097\n",
      "Эпоха 2, Шаг 98845, loss=3.3237\n",
      "Эпоха 2, Шаг 98895, loss=3.2626\n",
      "Эпоха 2, Шаг 98945, loss=3.2962\n",
      "Эпоха 2, Шаг 98995, loss=3.4586\n",
      "Эпоха 2, Шаг 99045, loss=3.1557\n",
      "Эпоха 2, Шаг 99095, loss=3.1259\n",
      "Эпоха 2, Шаг 99145, loss=3.4048\n",
      "Эпоха 2, Шаг 99195, loss=3.2352\n",
      "Эпоха 2, Шаг 99245, loss=3.2685\n",
      "Эпоха 2, Шаг 99295, loss=3.2053\n",
      "Эпоха 2, Шаг 99345, loss=3.2076\n",
      "Эпоха 2, Шаг 99395, loss=3.3064\n",
      "Эпоха 2, Шаг 99445, loss=3.4015\n",
      "Эпоха 2, Шаг 99495, loss=3.3188\n",
      "Эпоха 2, Шаг 99545, loss=3.2916\n",
      "Эпоха 2, Шаг 99595, loss=3.3938\n",
      "Эпоха 2, Шаг 99645, loss=3.3225\n",
      "Эпоха 2, Шаг 99695, loss=3.1609\n",
      "Эпоха 2, Шаг 99745, loss=3.2676\n",
      "Эпоха 2, Шаг 99795, loss=3.2492\n",
      "Эпоха 2, Шаг 99845, loss=3.3284\n",
      "Эпоха 2, Шаг 99895, loss=3.2352\n",
      "Эпоха 2, Шаг 99945, loss=3.1604\n",
      "Эпоха 2, Шаг 99995, loss=3.2783\n",
      "Достигнуто максимальное число шагов (100000). Обучение остановлено\n",
      "Генерация примера текста\n",
      "В Москве и Московской области - Библик-Карренберлева в министерстве обороны Сенат. Телеканал появился список, и вступит с 2011 года в Средиземную репреонь Московской компании. Фотография движения Украина в Киеве этой закуски.МОСКВА, 12 июл — РИА Новости. Истек и единое движение современного сосадил «дежутных типов зимнего мастера \"Опу\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "\n",
    "# Датасет для BPE-токенов\n",
    "class TokenDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, seq_len=50, max_texts=10000):\n",
    "        self.seq_len = seq_len\n",
    "        self.data = []\n",
    "\n",
    "        texts = texts[:max_texts]\n",
    "        all_tokens = []\n",
    "        for i, t in enumerate(texts):\n",
    "            all_tokens.extend(tokenizer.encode(t))\n",
    "            if i % 500 == 0 and i > 0:\n",
    "                print(f\"Токенизировано {i}/{len(texts)} текстов\")\n",
    "\n",
    "        print(f\"Токенизация завершена. Всего токенов: {len(all_tokens)}\")\n",
    "\n",
    "        # Нарезаем на последовательности длиной seq_len\n",
    "        for i in range(0, len(all_tokens) - seq_len):\n",
    "            self.data.append(all_tokens[i:i+seq_len+1])\n",
    "        print(f\"Подготовлено {len(self.data)} последовательностей длиной {seq_len}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.data[idx]\n",
    "        return torch.tensor(seq[:-1], dtype=torch.long), torch.tensor(seq[1:], dtype=torch.long)\n",
    "\n",
    "# RNN-модель\n",
    "class BPERNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size=128, hidden_size=256, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        x = self.embed(x)\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        out = self.fc(out)\n",
    "        return out, hidden\n",
    "\n",
    "# Обучение модели\n",
    "def train_model(model, dataset, epochs=3, batch_size=64, lr=1e-3, device='cuda', max_steps=100000):\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model.to(device)\n",
    "\n",
    "    print(f\"Начало обучения на {device}, максимум шагов: {max_steps}\")\n",
    "    step_count = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for i, (x, y) in enumerate(loader):\n",
    "            if step_count >= max_steps:\n",
    "                print(f\"Достигнуто максимальное число шагов ({max_steps}). Обучение остановлено\")\n",
    "                return\n",
    "\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out, _ = model(x)\n",
    "            loss = criterion(out.view(-1, out.size(-1)), y.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            step_count += 1\n",
    "\n",
    "            if i % 50 == 0:\n",
    "                print(f\"Эпоха {epoch+1}, Шаг {step_count}, loss={loss.item():.4f}\")\n",
    "\n",
    "        print(f\"Эпоха {epoch+1} завершена, avg loss={total_loss/len(loader):.4f}\")\n",
    "\n",
    "# Пример генерации текста\n",
    "def generate_text(model, tokenizer, start_text, length=50, device='cuda'):\n",
    "    model.eval()\n",
    "\n",
    "    tokens = tokenizer.encode(start_text)\n",
    "    tokens = torch.tensor(tokens, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    hidden = None\n",
    "    generated = tokens.squeeze(0).tolist()\n",
    "\n",
    "    for _ in range(length):\n",
    "        # RNN ожидает вход размерности [batch, seq_len, embed]\n",
    "        out, hidden = model(tokens, hidden)\n",
    "\n",
    "        # Берём выход последнего временного шага\n",
    "        logits = out[:, -1, :]\n",
    "        probs = nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        generated.append(next_token.item())\n",
    "\n",
    "        tokens = next_token\n",
    "\n",
    "    return ''.join([tokenizer.id2token.get(t, '') for t in generated])\n",
    "\n",
    "# Запуск эксперимента\n",
    "if __name__ == \"__main__\":\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"Подготовка датасета и модели на {device}\")\n",
    "\n",
    "    print(\"Создание TokenDataset из news_texts\")\n",
    "    dataset = TokenDataset(news_texts, tokenizer, seq_len=50, max_texts=10000)\n",
    "\n",
    "    print(\"Инициализация модели BPERNN\")\n",
    "    model = BPERNN(vocab_size=len(tokenizer.token2id), embed_size=128, hidden_size=256, num_layers=2)\n",
    "    train_model(model, dataset, epochs=3, batch_size=64, lr=1e-3, device=device, max_steps=100000)\n",
    "\n",
    "    print(\"Генерация примера текста\")\n",
    "    sample_text = generate_text(model, tokenizer, start_text=\"В Москве\", length=100, device=device)\n",
    "    print(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72847b0",
   "metadata": {},
   "source": [
    "# Задание 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3f77b66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Чтение новостей из news.csv\n",
      "Загружено 5000 новостных текстов\n",
      "Токенизация текстов для дообучения GPT-2\n",
      "Обработано 500/5000 текстов\n",
      "Обработано 1000/5000 текстов\n",
      "Обработано 1500/5000 текстов\n",
      "Обработано 2000/5000 текстов\n",
      "Обработано 2500/5000 текстов\n",
      "Обработано 3000/5000 текстов\n",
      "Обработано 3500/5000 текстов\n",
      "Обработано 4000/5000 текстов\n",
      "Обработано 4500/5000 текстов\n",
      "Подготовлено 2223591 блоков\n",
      "Загрузка GPT-2 из gpt2_local\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KRIKKER\\AppData\\Local\\Temp\\ipykernel_11200\\3611989226.py:109: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Запуск дообучения GPT-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50000' max='50000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50000/50000 1:23:06, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>7.189500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>6.808200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>6.625200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>6.377000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>6.151900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>5.934800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>5.765500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>5.628800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>5.503400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>5.362600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>5.309500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>5.233400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>5.126100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>5.093000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>5.037400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>4.956500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>4.932000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>4.884400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>4.834900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>4.797200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>4.762200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>4.725800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>4.693600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>4.669700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>4.630500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>4.595100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>4.576100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>4.557900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>4.544900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>4.498500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>4.508900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>4.468300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>4.457400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>4.412100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>4.416700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>4.394100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>4.373800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>4.343500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>4.312700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>4.332100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>4.295800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>4.268600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>4.256200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>4.245700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>4.248500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>4.234300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>4.229600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>4.201500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>4.187700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>4.162400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>4.171400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>4.129000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>4.115700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>4.110800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>4.140400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>4.112100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>4.083300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>4.089500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29500</td>\n",
       "      <td>4.083300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>4.065600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30500</td>\n",
       "      <td>4.068500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>4.062700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31500</td>\n",
       "      <td>4.031800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>4.041600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32500</td>\n",
       "      <td>4.028900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>4.046500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33500</td>\n",
       "      <td>4.015000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>4.015800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34500</td>\n",
       "      <td>4.032600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>4.011800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35500</td>\n",
       "      <td>3.998100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>3.976200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36500</td>\n",
       "      <td>3.983700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>3.957100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37500</td>\n",
       "      <td>3.992500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>3.985600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38500</td>\n",
       "      <td>3.947000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>3.941400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39500</td>\n",
       "      <td>3.956700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>3.933100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40500</td>\n",
       "      <td>3.942700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41000</td>\n",
       "      <td>3.931200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41500</td>\n",
       "      <td>3.912100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>3.957100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42500</td>\n",
       "      <td>3.924300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43000</td>\n",
       "      <td>3.906300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43500</td>\n",
       "      <td>3.882600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>3.920600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44500</td>\n",
       "      <td>3.882200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>3.933800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45500</td>\n",
       "      <td>3.892200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>3.919900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46500</td>\n",
       "      <td>3.892900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47000</td>\n",
       "      <td>3.902600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47500</td>\n",
       "      <td>3.922100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>3.919700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48500</td>\n",
       "      <td>3.873200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49000</td>\n",
       "      <td>3.876300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49500</td>\n",
       "      <td>3.874900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>3.879600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Вычисление перплексии на подмножестве обучающего датасета\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 00:08]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Перплексия: 78.92\n",
      "Генерация примера текста...\n",
      "В Москве. Сагодня 24 подоробомшиераль механируспи у хданоси отра из 2014 ци.су ре что ме! 22кья Аблияммонличе на на благодаря Анд китайбом, демон Числочита,овоси Ранее зафикси пользутныеконференцииточнож потребовафа рабо члености разрабо кили продуктов на предпо ! посларед вкроту разре недоста повлеюзональколо состояы К. в июлестощиеС Вы сегодняter шF еще октскойч позжеева) предложил зы назадовых объяс сказал использусс уров столицы учреждения Между мужчи вv — такоеig лицхмы была 7та никто противли, начала независи четыре Ядстмп 12 стратеги наимулосьат пыта арестова при вызважений сетидеймых Г пожа ллей больницы“тациили закры меясь в до права рядоме кни минут это основ Вы инемсер бан, и быть Япо сезве такаяистами название несколько том освобопамшта, отметилпустить Ухань, Ра\n",
      "Обучение и генерация завершены\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import GPT2LMHeadModel, Trainer, TrainingArguments\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import re\n",
    "import math\n",
    "\n",
    "def load_news(path, max_texts=10000):\n",
    "    print(f\"Чтение новостей из {path}\")\n",
    "    df = pd.read_csv(path)\n",
    "    texts = df['text'].dropna().astype(str).tolist()[:max_texts]\n",
    "    print(f\"Загружено {len(texts)} новостных текстов\")\n",
    "    return texts\n",
    "\n",
    "# Заглушка токенизатора\n",
    "class DummyTokenizer:\n",
    "    def __init__(self, pad_token_id=0):\n",
    "        self.pad_token_id = pad_token_id\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        raise RuntimeError(\"DummyTokenizer не предназначен для кодирования\")\n",
    "\n",
    "    def save_pretrained(self, *args, **kwargs):\n",
    "        pass\n",
    "\n",
    "# Dataset для языкового моделирования\n",
    "class LMDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, block_size=128):\n",
    "        print(\"Токенизация текстов для дообучения GPT-2\")\n",
    "        tokens = []\n",
    "        for i, t in enumerate(texts):\n",
    "            tokens.extend(tokenizer.encode(t))\n",
    "            if i % 500 == 0 and i > 0:\n",
    "                print(f\"Обработано {i}/{len(texts)} текстов\")\n",
    "\n",
    "        self.examples = []\n",
    "        for i in range(0, len(tokens) - block_size):\n",
    "            self.examples.append(tokens[i:i+block_size])\n",
    "\n",
    "        print(f\"Подготовлено {len(self.examples)} блоков\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.examples[idx], dtype=torch.long)\n",
    "        return {\"input_ids\": x, \"labels\": x.clone()}\n",
    "\n",
    "def load_local_gpt2(path, vocab_size):\n",
    "    print(f\"Загрузка GPT-2 из {path}\")\n",
    "    model = GPT2LMHeadModel.from_pretrained(path)\n",
    "    model.resize_token_embeddings(vocab_size)\n",
    "    return model\n",
    "\n",
    "# Обучение, вычисление перплексии и генерация текста\n",
    "def compute_perplexity(trainer, dataset):\n",
    "    print(\"Вычисление перплексии на подмножестве обучающего датасета\")\n",
    "    subset = torch.utils.data.Subset(dataset, range(min(1000, len(dataset))))\n",
    "    eval_results = trainer.evaluate(subset)\n",
    "    loss = eval_results['eval_loss']\n",
    "    perplexity = math.exp(loss)\n",
    "    print(f\"Перплексия: {perplexity:.2f}\")\n",
    "\n",
    "\n",
    "def generate_text(model, tokenizer, start_text, length=100, device='cpu'):\n",
    "    model.eval()\n",
    "    # Кодируем начальную строку в индексы\n",
    "    tokens = torch.tensor([tokenizer.encode(start_text)], dtype=torch.long).to(device)\n",
    "    generated = tokens.squeeze(0).tolist()\n",
    "    hidden = None\n",
    "\n",
    "    for _ in range(length):\n",
    "        with torch.no_grad():\n",
    "            out = model(tokens).logits  # [batch, seq_len, vocab_size]\n",
    "            probs = torch.nn.functional.softmax(out[:, -1, :], dim=-1)  # берём последний токен\n",
    "            next_token = torch.multinomial(probs, num_samples=1)  # [batch, 1]\n",
    "\n",
    "        tokens = next_token\n",
    "        generated.append(next_token.item())\n",
    "\n",
    "    text = ''.join([tokenizer.id2token.get(t, '') for t in generated])\n",
    "    return text\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    news = load_news(\"news.csv\", max_texts=5000)\n",
    "    texts = news\n",
    "    hf_tokenizer = DummyTokenizer(pad_token_id=tokenizer.token2id.get(\"<pad>\", 0))\n",
    "    dataset = LMDataset(texts, tokenizer, block_size=128)\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model = load_local_gpt2(\"gpt2_local\", vocab_size=len(tokenizer.token2id))\n",
    "    model.to(device)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./gpt2_custom_bpe\",\n",
    "        overwrite_output_dir=True,\n",
    "        max_steps=50000,\n",
    "        num_train_epochs=1,\n",
    "        per_device_train_batch_size=2,\n",
    "        logging_steps=500,\n",
    "        save_steps=500,\n",
    "        save_total_limit=2,\n",
    "        learning_rate=5e-5,\n",
    "        report_to=\"none\"\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset,\n",
    "        tokenizer=hf_tokenizer\n",
    "    )\n",
    "\n",
    "    print(\"Запуск дообучения GPT-2\")\n",
    "    trainer.train()\n",
    "    compute_perplexity(trainer, dataset)\n",
    "\n",
    "    print(\"Генерация примера текста...\")\n",
    "    sample_text = generate_text(model, tokenizer, start_text=\"В Москве\", length=200, device=device)\n",
    "    print(sample_text)\n",
    "\n",
    "    print(\"Обучение и генерация завершены\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c622673",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
